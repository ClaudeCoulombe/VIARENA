{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ca2b1d2d",
      "metadata": {
        "id": "ca2b1d2d"
      },
      "source": [
        "<a style=\"float:left;\" href=\"https://colab.research.google.com/github/ClaudeCoulombe/VIARENA/blob/master/Labos/Lab-Detection_Objets/Detection_Objets-colab.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<br/>\n",
        "### Rappel - Fonctionnement d'un carnet web iPython\n",
        "\n",
        "* Pour exécuter le code contenu dans une cellule d'un carnet iPython, cliquez dans la cellule et faites (⇧↵, shift-enter) \n",
        "* Le code d'un carnet iPython s'exécute séquentiellement de haut en bas de la page. Souvent, l'importation d'une bibliothèque Python ou l'initialisation d'une variable est préalable à l'exécution d'une cellule située plus bas. Il est donc recommandé d'exécuter les cellules en séquence. Enfin, méfiez-vous des retours en arrière qui peuvent réinitialiser certaines variables.\n",
        "* Pour obtenir de l'information sur une fonction, utilisez la commande Python `help(`\"nom de la fonction\"`)`\n",
        "\n",
        "SVP, déployez toutes les cellules en sélectionnant l'item « Développer les rubriques » de l'onglet « Affichage »."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7dd840",
      "metadata": {
        "id": "4c7dd840"
      },
      "source": [
        "# Détection d'objets\n",
        "\n",
        "Bien que manquant de temps et d'espace pour traiter sérieusement de la tâche de détection d'objets, voici un petit labo Colab «en prime» pour vous mettre en appétit sur ce sujet plein d'applications.\n",
        "\n",
        "Rappelons que la détection consiste à identifier les différents objets dans une image en les encadrant dans un rectangle.\n",
        "\n",
        "La détection d'objets est utilisée pour identifier, isoler et même compter des objets dans une image, cela inclut le comptage du bétail dans un enclos, l'estimation du nombre d'arbres dans une forêt, ou le comptage du nombre de poissons dans une passe migratoire."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43122d47",
      "metadata": {
        "id": "43122d47"
      },
      "source": [
        "## Modèle créé à l'aide de la bibliothèque spécialisée en détection d'objets de TensorFlow\n",
        "\n",
        "<p>Utilisation de la <a href=\"https://github.com/tensorflow/models/tree/master/research/object_detection\" target='_blank'>bibliothèque spécialisée en détection d'objets de TensorFlow</a> et de son IPA (<i>API</i>) de haut niveau. La bibliothèque de détection d'objets TensorFlow utilise l'algorithme (<i>Faster R-CNN</i>) avec un extracteur d'attributs visuels basé sur l'architecture de réseau convolutif profond <a href=\"https://paperswithcode.com/method/inception-resnet-v2\" target='_blank'>Inception Resnet v2</a>. Le modèle est entraîné par apprentissage par transfert à partir d'un modèle préentraîné sur Imagenet puis peaufiné sur l'ensemble de données <a href=\"https://cocodataset.org/#home\" target='_blank'>COCO</a> 2017 avec des lots de 64 images en couleur de 640 par 640 pixels.</p>\n",
        "\n",
        "### L'algorithme <code>Faster R-CNN</code>\n",
        "\n",
        "Voici une tentative d'explication des étapes principales de l'algorithme <code>Faster R-CNN</code>:\n",
        "\n",
        "<ul>\n",
        "    <ol>\n",
        "        <li>Un réseau convolutif profond, d'architecture <a href=\"https://paperswithcode.com/method/inception-resnet-v2\" target='_blank'>Inception Resnet v2</a>, extrait une représentation des attributs visuels de l’image d'entrée;</li> \n",
        "        <li>Cette représentation passe à un réseau dit de proposition de régions (en anglais, <i>region proposal network</i> ou <i>RPN</i>), qui génère des propositions de régions qui sont des rectangles qui délimitent les objets pertinents de l’image. Pour cela, le réseau de proposition de régions utilise des rectangles de taille fixe placés uniformément sur l’image originale afin de détecter les objets par balayage;</li>\n",
        "        <li>Les régions candidates sont ensuite filtrées par un algorithme dit de suppression des non-maxima (en anglais, <i>non-maximum suppression</i>), appelé aussi <a href=\"https://fr.wikipedia.org/wiki/Filtre_de_Canny\" target='_blank'>filtre de Canny</a>, qui ne conserve que les régions les plus prometteuses en se basant sur la netteté de leurs contours;</li> \n",
        "        <li>La représentation des attributs visuels de l’image d'entrée 1) et les rectangles de délimitation des objets pertinents 3) sont combinés pour générer une nouvelle représentation par sous-échantillonnage (<i>pooling</i>) des régions d'intérêt (en anglais, <i>regions of interest</i> ou <i>RoI</i>);</li>\n",
        "        <li>Finalement, la représentation des régions d'intérêt passe dans un perceptron multicouche pour la prédiction des coordonnées (par régression) des rectangles contenant des objets et la prédiction (par classification softmax) des classes d'objets.</li> \n",
        "    </ol>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3efa1e08",
      "metadata": {
        "id": "3efa1e08"
      },
      "source": [
        "### Inspiration et droits d'auteur\n",
        "\n",
        "Ce laboratoire est une adaptation et modification directe du tutoriel <a href=\"https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1\" target='_blank'>TensorFlow Hub Object Detection</a> - site Google / TensorFlow Hub\n",
        "\n",
        "##### Copyright (c) 2020, The TensorFlow Hub Authors.\n",
        "##### Copyright (c) 2022, Claude COULOMBE, modifications et adaptation française\n",
        "\n",
        "Le contenu de cette page est sous licence <a href=\"https://creativecommons.org/licenses/by/3.0/deed.fr\" target='_blank'>Creative Commons Attribution 3.0 (CC BY 3.0)</a>,<br/>et les exemples de code sont sous <a href=\"https://www.apache.org/licenses/LICENSE-2.0\" target='_blank'>licence Apache 2.0</a>.\n",
        "\n",
        "#### Données\n",
        "\n",
        "Les données sur les images du jeux de données <a href=\"https://cocodataset.org/#home\" target='_blank'>COCO</a> (<i>Common Objects in Context</i>) de Microsoft, une banque en données ouvertes sous licence <a href=\"https://creativecommons.org/licenses/by/4.0/deed.fr\" target='_blank'>Creative Commons Attribution 4.0 (CC BY 4.0)</a> de 330 000 photos de 80 objets communs dans leur contexte.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4234c744",
      "metadata": {
        "id": "4234c744"
      },
      "source": [
        "## Fixer le hasard pour la reproductibilité\n",
        "\n",
        "La mise au point de réseaux de neurones implique certains processus aléatoires. Afin de pouvoir reproduire et comparer vos résultats d'expérience, vous fixez temporairement l'état aléatoire grâce à un germe aléatoire unique.\n",
        "\n",
        "Pendant la mise au point, vous fixez temporairement l'état aléatoire pour la reproductibilité mais vous répétez l'expérience avec différents germes ou états aléatoires et prenez la moyenne des résultats.\n",
        "<br/>\n",
        "**Note** : Pour un système en production, vous ravivez simplement l'état  purement aléatoire avec l'instruction `GERME_ALEATOIRE = None`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "008660d4",
      "metadata": {
        "id": "008660d4"
      },
      "outputs": [],
      "source": [
        "# This Colab requires TF 2.5.\n",
        "# !pip3 install -U \"tensorflow>=2.5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dde8782e",
      "metadata": {
        "id": "dde8782e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Version TensorFlow:\",tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6d1cafd",
      "metadata": {
        "id": "c6d1cafd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Définir un germe aléatoire\n",
        "GERME_ALEATOIRE = 42\n",
        "\n",
        "# Définir un état aléatoire pour Python\n",
        "os.environ['PYTHONHASHSEED'] = str(GERME_ALEATOIRE)\n",
        "\n",
        "# Définir un état aléatoire pour Python random\n",
        "import random\n",
        "random.seed(GERME_ALEATOIRE)\n",
        "\n",
        "# Définir un état aléatoire pour NumPy\n",
        "import numpy as np\n",
        "np.random.seed(GERME_ALEATOIRE)\n",
        "\n",
        "# Définir un état aléatoire pour TensorFlow\n",
        "\n",
        "tf.random.set_seed(GERME_ALEATOIRE)\n",
        "\n",
        "# Note: Retrait du comportement déterministe\n",
        "# à cause de keras.layers.RandomContrast(...)\n",
        "# dont il n'existe pas de version déterministe\n",
        "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "# os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "print(\"Germe aléatoire fixé\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e651846",
      "metadata": {
        "id": "9e651846"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "print(\"Bibliothèques importées\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceff4ea3",
      "metadata": {
        "id": "ceff4ea3"
      },
      "source": [
        "## Utilitaires"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455ef4b3",
      "metadata": {
        "id": "455ef4b3"
      },
      "source": [
        "Exécutez le code de la cellule suivante pour créer des utilitaires\n",
        "\n",
        "* Méthode pour charger une image\n",
        "* Liste de tous les modèles de TensorFlow Hub\n",
        "* Liste des tuples avec les points d'ancrage pour des poses humaines pour le jeu de données COCO 2017. Ceci est nécessaire pour les modèles impliquant des humains dont on analyse les poses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a67ec9f3",
      "metadata": {
        "id": "a67ec9f3"
      },
      "outputs": [],
      "source": [
        "def lecture_image(chemin_vers_fichier):\n",
        "    \"\"\"\n",
        "    Lecture et chargement d'une image à partir d'un fichier \n",
        "    local ou moissonné sur la Toile. L'image est stockée dans\n",
        "    dans un tableau NumPy de dimensions (hauteur,largeur,canaux)\n",
        "    où canaux=3 pour une image couleur RVB.\n",
        "    Arguments:\n",
        "       le chemin vers le fichier de l'image\n",
        "    Sortie:\n",
        "       un tableau Numpy (image_hauteur,image_largeur,3)\n",
        "    \"\"\"\n",
        "    image = None\n",
        "    if(chemin_vers_fichier.startswith('http')):\n",
        "        reponse = urlopen(chemin_vers_fichier)\n",
        "        image = reponse.read()\n",
        "        image = BytesIO(image)\n",
        "        image = Image.open(image)\n",
        "    else:\n",
        "        image = tf.io.gfile.GFile(chemin_vers_fichier, 'rb').read()\n",
        "        image = Image.open(BytesIO(image))\n",
        "    (image_largeur, image_hauteur) = image.size\n",
        "    return np.array(image.getdata()).reshape((1, image_hauteur, image_largeur, 3)).astype(np.uint8)\n",
        "\n",
        "TOUS_LES_MODELES_DU_HUB_TENSORFLOW = {\n",
        "    'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n",
        "    'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n",
        "    'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n",
        "    'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n",
        "    'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n",
        "    'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n",
        "    'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n",
        "    'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n",
        "    'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n",
        "    'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
        "    'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n",
        "    'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n",
        "    'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n",
        "    'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n",
        "    'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n",
        "    'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n",
        "    'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n",
        "    'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n",
        "    'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n",
        "    'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n",
        "    'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n",
        "    'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n",
        "    'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n",
        "    'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n",
        "    'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n",
        "    'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n",
        "    'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n",
        "    'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
        "    'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n",
        "    'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n",
        "    'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
        "    'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n",
        "    'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n",
        "    'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n",
        "    'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n",
        "    'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n",
        "    'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
        "    'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n",
        "    'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
        "}\n",
        "\n",
        "IMAGES_TEST = {\n",
        "  \"Plage\" : \"models/research/object_detection/test_images/image2.jpg\",\n",
        "  \"Chiens\" : \"models/research/object_detection/test_images/image1.jpg\",\n",
        "  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n",
        "  \"Taverne à Naxos\" : \"https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg\",\n",
        "  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n",
        "  \"Bibittes\" : \"https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg\",\n",
        "  # By Américo Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n",
        "  \"Téléphones\" : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n",
        "  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n",
        "  'Oiseaux' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n",
        "  # Source: https://images.radio-canada.ca/q_auto,w_960/v1/ici-info/16x9/homard-iles-de-la-madeleine-mise-e-leau-saison-peche-3.jpg,\n",
        "  # Deux ajouts sur la thématiue des pêches\n",
        "  \"Bateaux\" : \"https://images.radio-canada.ca/q_auto,w_960/v1/ici-info/16x9/homard-iles-de-la-madeleine-mise-e-leau-saison-peche-3.jpg\",\n",
        "  # Source: https://images.radio-canada.ca/q_auto,w_960/v1/ici-info/16x9/pecheurs-iles-madeleine-peche-grosse-ile-port-peche-debarquement.JPG,\n",
        "  \"Pêcheurs\" : \"https://images.radio-canada.ca/q_auto,w_960/v1/ici-info/16x9/pecheurs-iles-madeleine-peche-grosse-ile-port-peche-debarquement.jpg\",\n",
        "}\n",
        "\n",
        "COCO17_POINTS_ANCRAGE_POSE_HUMAINE = [(0, 1),\n",
        "                                      (0, 2),\n",
        "                                      (1, 3),\n",
        "                                      (2, 4),\n",
        "                                      (0, 5),\n",
        "                                      (0, 6),\n",
        "                                      (5, 7),\n",
        "                                      (7, 9),\n",
        "                                      (6, 8),\n",
        "                                      (8, 10),\n",
        "                                      (5, 6),\n",
        "                                      (5, 11),\n",
        "                                      (6, 12),\n",
        "                                      (11, 12),\n",
        "                                      (11, 13),\n",
        "                                      (13, 15),\n",
        "                                      (12, 14),\n",
        "                                      (14, 16)\n",
        "                                      ]\n",
        "\n",
        "print(\"Utilitaires chargés\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cf0b665",
      "metadata": {
        "id": "7cf0b665"
      },
      "source": [
        "## Outils de visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fb647af",
      "metadata": {
        "id": "0fb647af"
      },
      "source": [
        "Pour visualiser les images avec les rectangles, les points d'ancrage et les étiquettes appropriés, nous utiliserons la bibliothèque spécialisée en détection d'objets de TensorFlow et son IPA (<i>API</i>). Pour l'installer, nous allons cloner le référentiel GitHub des modèles TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a55790c8",
      "metadata": {
        "id": "a55790c8"
      },
      "outputs": [],
      "source": [
        "# cloner le dépôt GitHub des modèles TensorFlow.\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de310463",
      "metadata": {
        "id": "de310463"
      },
      "source": [
        "### Installation de la bibliothèque spécialisée en détection d'objets de TensorFlow "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "579d6183",
      "metadata": {
        "id": "579d6183"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python3 -m pip install .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maintenant, nous allons importer quelques modules dont nous aurons besoin plus tard"
      ],
      "metadata": {
        "id": "8yc2kYCb6OQ9"
      },
      "id": "8yc2kYCb6OQ9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183f0534",
      "metadata": {
        "id": "183f0534"
      },
      "outputs": [],
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Cellule exécutée\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Charger les données sur les étiquettes de classe (pour l'affichage)\n",
        "\n",
        "Pour l'affichage des étiquettes des objets détectés, nous utilisons un dictionnaire des étiquettes des 80 différentes classes d'objets du jeu de données COCO.\n",
        "\n",
        "Le dictionnaire des étiquettes, `index_etiquettes_classes` contient les numéros d'index, chacun associé à un nom de classe ou catégories, de sorte que lorsque notre réseau de convolution prédit le nombre 5, nous savons que cela correspond à la classe 'avion'. Ici, nous utilisons des fonctions utilitaires internes, mais tout ce qui renvoie un dictionnaire reliant l'index aux étiquettes (i.e. chaînes de caractères) appropriées conviendrait.\n",
        "\n",
        "Nous allons, pour plus de simplicité, charger le dictionnaire `index_etiquettes_classes` depuis le répertoire où nous avons chargé le code de la bibliothèque de détection d'objets TensorFlow."
      ],
      "metadata": {
        "id": "GQveHYOS6fX_"
      },
      "id": "GQveHYOS6fX_"
    },
    {
      "cell_type": "code",
      "source": [
        "CHEMIN_VERS_ETIQUETTES_DE_CLASSES = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "index_etiquettes_classes = label_map_util.create_category_index_from_labelmap(CHEMIN_VERS_ETIQUETTES_DE_CLASSES, use_display_name=True)\n",
        "index_etiquettes_classes"
      ],
      "metadata": {
        "id": "ewMV4EcI6TCn"
      },
      "id": "ewMV4EcI6TCn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Traduction en français des étiquettes de classe (pour l'affichage)\n",
        "\n",
        "Bien qu'il aurait été possible de remplacer directement le dictionnaire `index_etiquettes_classes` par une version traduite, les bonnes pratiques de génie logiciel nous invitent à le faire plutôt à la volée à partir d'un dictionnaire de traduction `dict_traductions_francaises` ce qui assure une plus grande modularité en prévision de changements.\n"
      ],
      "metadata": {
        "id": "wbYN61nV_KAN"
      },
      "id": "wbYN61nV_KAN"
    },
    {
      "cell_type": "code",
      "source": [
        "dict_traductions_francaises = { \"person\" : \"personne\",\n",
        "                                \"bicycle\" : \"vélo\",\n",
        "                                \"car\" : \"auto\",\n",
        "                                \"motorcycle\" : \"moto\",\n",
        "                                \"airplane\" : \"avion\",\n",
        "                                \"bus\" : \"bus\",\n",
        "                                \"train\" : \"train\",\n",
        "                                \"truck\" : \"camion\",\n",
        "                                \"boat\" : \"bateau\",\n",
        "                                \"traffic light\" : \"feu de circulation\",\n",
        "                                \"fire hydrant\" : \"borne d'incendie\",\n",
        "                                \"stop sign\" : \"panneau d'arrêt\",\n",
        "                                \"parking meter\" : \"parcomètre\",\n",
        "                                \"bench\" : \"banc\",\n",
        "                                \"bird\" : \"oiseau\",\n",
        "                                \"cat\" : \"chat\",\n",
        "                                \"dog\" : \"chien\",\n",
        "                                \"horse\" : \"cheval\",\n",
        "                                \"sheep\" : \"mouton\",\n",
        "                                \"cow\" : \"vache\",\n",
        "                                \"elephant\" : \"éléphant\",\n",
        "                                \"bear\" : \"ours\",\n",
        "                                \"zebra\" : \"zèbre\",\n",
        "                                \"giraffe\" : \"girafe\",\n",
        "                                \"backpack\" : \"sac à dos\",\n",
        "                                \"umbrella\" : \"parapluie\",\n",
        "                                \"handbag\" : \"sac à main\",\n",
        "                                \"tie\" : \"cravate\",\n",
        "                                \"suitcase\" : \"valise\",\n",
        "                                \"frisbee\" : \"frisbee\",\n",
        "                                \"skis\" : \"skis\",\n",
        "                                \"snowboard\" : \"planche à neige\",\n",
        "                                \"sports ball\" : \"ballon de sport\",\n",
        "                                \"kite\" : \"cerf-volant\",\n",
        "                                \"baseball bat\" : \"batte de baseball\",\n",
        "                                \"baseball glove\" : \"gant de baseball\",\n",
        "                                \"skateboard\" : \"planche à roulette\",\n",
        "                                \"surfboard\" : \"planche de surf\",\n",
        "                                \"tennis racket\" : \"raquette de tennis\",\n",
        "                                \"bottle\" : \"bouteille\",\n",
        "                                \"wine glass\" : \"coupe à vin\",\n",
        "                                \"cup\" : \"tasse\",\n",
        "                                \"fork\" : \"fourchette\",\n",
        "                                \"knife\" : \"couteau\",\n",
        "                                \"spoon\" : \"cuillère\",\n",
        "                                \"bowl\" : \"bol\",\n",
        "                                \"banana\" : \"banane\",\n",
        "                                \"apple\" : \"pomme\",\n",
        "                                \"sandwich\" : \"sandwich\",\n",
        "                                \"orange\" : \"orange\",\n",
        "                                \"broccoli\" : \"brocoli\",\n",
        "                                \"carrot\" : \"carotte\",\n",
        "                                \"hot dog\" : \"hot-dog\",\n",
        "                                \"pizza\" : \"pizza\",\n",
        "                                \"donut\" : \"beignet\",\n",
        "                                \"cake\" : \"gâteau\",\n",
        "                                \"chair\" : \"chaise\",\n",
        "                                \"couch\" : \"canapé\",\n",
        "                                \"potted plant\" : \"plante en pot\",\n",
        "                                \"bed\" : \"lit\",\n",
        "                                \"dining table\" : \"table à manger\",\n",
        "                                \"toilet\" : \"toilettes\",\n",
        "                                \"tv\" : \"télé\",\n",
        "                                \"laptop\" : \"ordinateur portable\",\n",
        "                                \"mouse\" : \"souris\",\n",
        "                                \"remote\" : \"commande à distance\",\n",
        "                                \"keyboard\" : \"clavier\",\n",
        "                                \"cell phone\" : \"téléphone portable\",\n",
        "                                \"microwave\" : \"four micro onde\",\n",
        "                                \"oven\" : \"four\",\n",
        "                                \"toaster\" : \"grille-pain\",\n",
        "                                \"sink\" : \"évier\",\n",
        "                                \"refrigerator\" : \"réfrigérateur\",\n",
        "                                \"book\" : \"livre\",\n",
        "                                \"clock\" : \"horloge\",\n",
        "                                \"vase\" : \"vase\",\n",
        "                                \"scissors\" : \"ciseaux\",\n",
        "                                \"teddy bear\" : \"ours en peluche\",\n",
        "                                \"hair drier\" : \"séchoir à cheveux\",\n",
        "                                \"toothbrush\" : \"brosse à dents\" \n",
        "                               }\n"
      ],
      "metadata": {
        "id": "rbIBMcaCgtWc"
      },
      "id": "rbIBMcaCgtWc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traduction du dictionnaire des étiquettes de classe \n",
        "for index in index_etiquettes_classes.keys():\n",
        "  index_etiquettes_classes[index]['name'] = dict_traductions_francaises[index_etiquettes_classes[index]['name']]\n",
        "\n",
        "print(\"Étiquettes de classes traduites\")"
      ],
      "metadata": {
        "id": "rRXjfqC1ET_s"
      },
      "id": "rRXjfqC1ET_s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construction  d'un modèle de détection et chargement des poids du modèle pré-entraîné\n",
        "\n",
        "Ici, vous choisissez le modèle de détection d'objets que vous utiliserez.\n",
        "Vous sélectionnez un architecture et elle sera chargée automatiquement.\n",
        "Si vous voulez changer de modèle pour essayer d'autres architectures, changez simplement la cellule de code ci-dessous et exécutez les cellules suivantes.\n",
        "\n",
        "**Astuce :** Pour obtnir plus de détails sur le modèle sélectionné, vous pouvez cliquez l'hyperlien vers la ressource associé au modèle et lire la documentation supplémentaire dans TF Hub. Après avoir sélectionné un modèle, nous afficherons l'hyperlien vers la ressource pour vous faciliter la tâche."
      ],
      "metadata": {
        "id": "7JdVMydI6y_K"
      },
      "id": "7JdVMydI6y_K"
    },
    {
      "cell_type": "code",
      "source": [
        "nom_du_modele = 'CenterNet HourGlass104 Keypoints 512x512' \n",
        "ressource_modele = TOUS_LES_MODELES_DU_HUB_TENSORFLOW[nom_du_modele]\n",
        "\n",
        "print('Nom du modèle: '+ nom_du_modele)\n",
        "print('Ressource du modèle dans TensorFlow Hub: {}'.format(ressource_modele))"
      ],
      "metadata": {
        "id": "CIpd3ueY6qSi"
      },
      "id": "CIpd3ueY6qSi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Charger le modèle sélectionné depuis TensorFlow Hub\n",
        "Ici, nous avons juste besoin de l'adresse (i.e, l'hyperlien) vers la ressource du modèle qui a été sélectionné et nous utilisons la bibliothèque Tensorflow Hub pour le charger en mémoire."
      ],
      "metadata": {
        "id": "9wW52YO97AVp"
      },
      "id": "9wW52YO97AVp"
    },
    {
      "cell_type": "code",
      "source": [
        "print('chargement du modèle...')\n",
        "modele = hub.load(ressource_modele)\n",
        "print('Modèle chargé!')"
      ],
      "metadata": {
        "id": "j65I63ue63mb"
      },
      "id": "j65I63ue63mb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture et affichage d'une image\n",
        "\n",
        "Maintenant, testez le modèle sur une image simple. Pour vous aider, nous fournissons une liste d'images de test dans la variable `IMAGES_TEST`.\n",
        "\n",
        "Pour les curieux, voici quelques trucs simples à essayer:\n",
        "\n",
        "* Essayez d'exécuter l'inférence sur vos propres images, téléchargez-les simplement sur Colab et chargez-les de la même manière que dans la cellule ci-dessous.\n",
        "\n",
        "* Modifiez les images d'entrée et voyez si la détection fonctionne toujours. Certaines choses simples à essayer ici incluent le retournement horizontal de l'image ou la conversion en niveaux de gris (notez que nous nous attendons toujours à ce que l'image d'entrée ait 3 canaux).\n",
        "\n",
        "**Attention :** Si vous utilisez des images avec un canal alpha, le modèle s'attend à des images à 3 canaux et l'alpha comptera comme un 4ème canal."
      ],
      "metadata": {
        "id": "h_OHGphC7MlI"
      },
      "id": "h_OHGphC7MlI"
    },
    {
      "cell_type": "code",
      "source": [
        "# ['Plage', 'Chiens', 'Taverne à Naxos', 'Bibittes', 'Téléphones', 'Oiseaux', 'Bateaux', 'Pêcheurs']\n",
        "choix_image = 'Bateaux'\n",
        "retourner_image_horizontalement = False \n",
        "convertir_image_gris = False\n",
        "\n",
        "chemin_vers_images = IMAGES_TEST[choix_image]\n",
        "image_numpy = lecture_image(chemin_vers_images)\n",
        "\n",
        "# Retourner l'image horizontalement \n",
        "if(retourner_image_horizontalement):\n",
        "  image_numpy[0] = np.fliplr(image_numpy[0]).copy()\n",
        "\n",
        "# Convertir l'image en niveaux de gris\n",
        "if(convertir_image_gris):\n",
        "  image_numpy[0] = np.tile(\n",
        "    np.mean(image_numpy[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_numpy[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iRntGwee7JX8"
      },
      "id": "iRntGwee7JX8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inférence - détection des objets\n",
        "\n",
        "Pour faire l'inférence (i.e. la détection d'objets), nous avons juste besoin d'appeler notre modèle chargé par TF Hub.\n",
        "\n",
        "Choses que vous pouvez essayer :\n",
        "\n",
        "* Imprimez `result['detection_boxes']` et essayez de faire correspondre les emplacements des boîtes aux boîtes de l'image. Notez que les coordonnées sont sous forme normalisée (c'est-à-dire dans l'intervalle [0, 1]).\n",
        "\n",
        "* Inspectez les autres clés du dictionnaire `resultats` (<i>dictionary keys</i>) de sortie des résultats. Une documentation complète peut être consultée sur la page de documentation des modèles (en consultant l'hyperlien associé au modèle affiché précédemment)."
      ],
      "metadata": {
        "id": "vOfTcYbw7ggf"
      },
      "id": "vOfTcYbw7ggf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Procéder à l'inférence (i.e. la détection des objets)\n",
        "resultats = modele(image_numpy)\n",
        "\n",
        "# dépendant du modèle de détection d'objets utilisé, le dictionnaire\n",
        "# contenant les résultats peut comporter des clés supplémentaires\n",
        "# cela est expliqué dans la documentation\n",
        "resultats = {key:value.numpy() for key,value in resultats.items()}\n",
        "print(resultats.keys())"
      ],
      "metadata": {
        "id": "_7VUUi3G7W4T"
      },
      "id": "_7VUUi3G7W4T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation des résultats\n",
        "\n",
        "Ici nous utilisons la bibliothèque de détection d'objets TensorFlow pour afficher les rectangles de l'étape d'inférence (et les points d'ancrage lorsqu'ils sont disponibles).\n",
        "\n",
        "La documentation complète de cette méthode peut être consultée [ici](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py)\n",
        "\n",
        "Ici, vous pouvez définir `min_score_thresh` entre 0 et 1 pour autoriser plus de détections ou pour filtrer plus de détections."
      ],
      "metadata": {
        "id": "Ivd0KHXc7yCU"
      },
      "id": "Ivd0KHXc7yCU"
    },
    {
      "cell_type": "code",
      "source": [
        "index_etiquette = 0\n",
        "image_numpy_avec_detections = image_numpy.copy()\n",
        "\n",
        "# Utilisez les points d'ancrage si disponible\n",
        "points_ancrage, point_ancrage_scores = None, None\n",
        "if 'detection_keypoints' in resultats:\n",
        "  points_ancrage = resultats['detection_keypoints'][0]\n",
        "  points_ancrage_scores = resultats['detection_keypoint_scores'][0]\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_numpy_avec_detections[0],\n",
        "      resultats['detection_boxes'][0],\n",
        "      (resultats['detection_classes'][0] + index_etiquette).astype(int),\n",
        "      resultats['detection_scores'][0],\n",
        "      index_etiquettes_classes,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=200,\n",
        "      #pré-réglage original 0.30\n",
        "      min_score_thresh=.20, \n",
        "      agnostic_mode=False,\n",
        "      keypoints=points_ancrage,\n",
        "      keypoint_scores=points_ancrage_scores,\n",
        "      keypoint_edges=COCO17_POINTS_ANCRAGE_POSE_HUMAINE)\n",
        "\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_numpy_avec_detections[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A6535LDe7mKA"
      },
      "id": "A6535LDe7mKA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fin de l'exécution du carnet IPython\")"
      ],
      "metadata": {
        "id": "d4Qye-KH71qr"
      },
      "id": "d4Qye-KH71qr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JoDxVTWN7_mC"
      },
      "id": "JoDxVTWN7_mC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "Detection_Objets-colab.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}