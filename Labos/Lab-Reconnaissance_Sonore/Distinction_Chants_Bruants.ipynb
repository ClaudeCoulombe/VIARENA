{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "81c5d990",
      "metadata": {
        "id": "81c5d990"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaudeCoulombe/VIARENA/blob/master/Labos/Lab-Reconnaissance_Sonore/Distinction_Chants_Bruants.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "### Rappel - Fonctionnement d'un carnet web iPython\n",
        "\n",
        "* Pour exécuter le code contenu dans une cellule d'un carnet iPython, cliquez dans la cellule et faites (⇧↵, shift-enter)\n",
        "* Le code d'un carnet iPython s'exécute séquentiellement de haut en bas de la page. Souvent, l'importation d'une bibliothèque Python ou l'initialisation d'une variable est préalable à l'exécution d'une cellule située plus bas. Il est donc recommandé d'exécuter les cellules en séquence. Enfin, méfiez-vous des retours en arrière qui peuvent réinitialiser certaines variables.\n",
        "\n",
        "<b>SVP</b>, déployez toutes les cellules en sélectionnant l'item « Développer les rubriques » de l'onglet « Affichage »."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "595e0445",
      "metadata": {
        "id": "595e0445"
      },
      "source": [
        "# Distinction des chants de Bruants\n",
        "## Labo avec des données sonores\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d16d23",
      "metadata": {
        "id": "87d16d23"
      },
      "source": [
        "## Inspiration et droits d'auteur\n",
        "\n",
        "Ce laboratoire est basé sur le Tutoriel TensorFlow - <a href=\"https://www.tensorflow.org/tutorials/audio/simple_audio?hl=fr\" target='_blank'>Reconnaissance audio simple / Reconnaître les mots-clés</a>.\n",
        "\n",
        "Copyright (c) 2019-2024, The TensorFlow Authors.\n",
        "\n",
        "Copyright (c) 2022-2024, Claude Coulombe\n",
        "\n",
        "Le contenu de cette page est sous licence <a href=\"https://creativecommons.org/licenses/by/4.0/deed.fr\" target='_blank'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>,<br/>et les exemples de code sont sous <a href=\"https://www.apache.org/licenses/LICENSE-2.0\" target='_blank'>licence Apache 2.0</a>.\n",
        "\n",
        "### Données\n",
        "\n",
        "Les données sur les chants d'oiseaux proviennent de de <a href=\"https://www.xeno-canto.org\" target='_blank'>Xeno-canto</a>, un site Web de partage d'enregistrements de sons d'oiseaux sauvages du monde entier, une banque en données ouvertes sous licence <i>Creative Commons</i>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccd1d275",
      "metadata": {
        "id": "ccd1d275"
      },
      "source": [
        "## Importation bibliothèques Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae752c6",
      "metadata": {
        "id": "5ae752c6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "print(\"Bibliothèques Python chargées\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f77e2cdf",
      "metadata": {
        "id": "f77e2cdf"
      },
      "source": [
        "## Fixer le hasard pour la reproductibilité\n",
        "\n",
        "La mise au point de réseaux de neurones implique certains processus aléatoires. Afin de pouvoir reproduire et comparer vos résultats d'expérience, vous fixez temporairement l'état aléatoire grâce à un germe aléatoire unique.\n",
        "\n",
        "Pendant la mise au point, vous fixez temporairement l'état aléatoire pour la reproductibilité mais vous répétez l'expérience avec différents germes ou états aléatoires et prenez la moyenne des résultats.\n",
        "<br/>\n",
        "##### **Note**: Pour un système en production, vous ravivez simplement l'état  purement aléatoire avec l'instruction `GERME_ALEATOIRE = None`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5042a274",
      "metadata": {
        "id": "5042a274"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Définir un germe aléatoire\n",
        "GERME_ALEATOIRE = 42\n",
        "\n",
        "# Définir un état aléatoire pour Python\n",
        "os.environ['PYTHONHASHSEED'] = str(GERME_ALEATOIRE)\n",
        "\n",
        "# Définir un état aléatoire pour Python random\n",
        "import random\n",
        "random.seed(GERME_ALEATOIRE)\n",
        "\n",
        "# Définir un état aléatoire pour NumPy\n",
        "import numpy as np\n",
        "np.random.seed(GERME_ALEATOIRE)\n",
        "\n",
        "# Définir un état aléatoire pour TensorFlow\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(GERME_ALEATOIRE)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "print(\"Germe aléatoire fixé\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fb9bb5d",
      "metadata": {
        "id": "1fb9bb5d"
      },
      "source": [
        "## Acquisition des données"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f65c088",
      "metadata": {
        "id": "7f65c088"
      },
      "source": [
        "### Jeu de données - cris et chants d'oiseaux Xeno-canto\n",
        "\n",
        "Ls données utilisées dans ce laboratoire proviennent de <a href=\"https://www.xeno-canto.org\" target='_blank'>Xeno-canto</a> (XC). Xeno-canto est un site Web de partage d'enregistrements de sons d'oiseaux sauvages du monde entier. Il a été lancé en 2005 par Bob Planqué et Willem-Pier Vellinga. Xeno-canto est géré par la fondation Xeno-canto (ou officiellement Stichting Xeno-canto voor natuurgeluiden ) des Pays-Bas.\n",
        "\n",
        "Notez que les données originalement en format .mp3 ont été converties en format .wav compatibles avec les outils TensorFlow.\n",
        "\n",
        "Les données sonores originales sont publiées sur Xeno-canto sous licence <i>Creative Commons</i>. Grâce à l'IPA ultrasimple de Xeno-canto, l'auteur d'un enregistrement est facile à retrouver à partir du nom original du fichier d'enregistrement. Par exemple, avec XC<numéro>.wav, il suffit de taper https://xeno-canto.org/<numéro> dans un fureteur pour retrouver les détails de l'enregistrement."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "849450da",
      "metadata": {
        "id": "849450da"
      },
      "source": [
        "Les <a href=\"https://www.kaggle.com/claudecoulombe/chants-de-bruants\" target='_blank'>données sur le chant des bruants</a>  sont téléchargeables à partir du site Kaggle. Mais vous allez utiliser l'IPA (<i>API</i>) de Kaggle pour accélérer les transferts de données."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f26108ec",
      "metadata": {
        "id": "f26108ec"
      },
      "source": [
        "### Création des répertoires de données\n",
        "\n",
        "Nous allons créer un répertoire de base `donnees`, un répertoire `lab_ecorces` où les données seront réparties en données d'entraînement, de validation et de test pour chaque classe cible.\n",
        "\n",
        "Enfin, un répertoire `modeles` pour mémoriser les modèles une fois entraînés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f67be87",
      "metadata": {
        "id": "4f67be87"
      },
      "outputs": [],
      "source": [
        "!mkdir donnees\n",
        "!mkdir \"donnees/sons_bruants\"\n",
        "chemin_donnees = \"donnees/sons_bruants\"\n",
        "repertoire_donnees = pathlib.Path(chemin_donnees)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ee05889",
      "metadata": {
        "id": "0ee05889"
      },
      "source": [
        "### Utilisation de l'IPA (<i>API</i>) de Kaggle pour l'importation directe du jeu de données BarkNet\n",
        "\n",
        "1. Commencez par installer la bibliothèque Python `kaggle`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80349cac",
      "metadata": {
        "id": "80349cac"
      },
      "outputs": [],
      "source": [
        "!pip3 install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0f7b0f",
      "metadata": {
        "id": "2c0f7b0f"
      },
      "source": [
        "2. Si ce n'est déjà fait, devenez membre de Kaggle avec votre adresse de courriel GMail:<br/>\n",
        "\n",
        "<img src=\"https://courses.edx.org/asset-v1:UMontrealX+Cegep-Matane-VIARENA+2T2024+type@asset+block@Kaggle_API-1.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b670a2aa",
      "metadata": {
        "id": "b670a2aa"
      },
      "source": [
        "3. Maintenant, vous devez télécharger votre clé privée pour utiliser l'IPA de Kaggle.\n",
        "\n",
        "4. Cliquez sur l'onglet « account » de votre profil Kaggle\n",
        "\n",
        "<img src=\"https://courses.edx.org/asset-v1:UMontrealX+Cegep-Matane-VIARENA+2T2024+type@asset+block@Kaggle_API-2.png\"/>\n",
        "\n",
        "5. Sur la page « Account » cliquez sur le bouton « Create New API Token ».\n",
        "    \n",
        "<img style=\"margin-left:40px;\" src=\"https://courses.edx.org/asset-v1:UMontrealX+Cegep-Matane-VIARENA+2T2024+type@asset+block@Kaggle_API-3.png\"/>\n",
        "\n",
        "6. Téléchargez votre clé privée « kaggle.json » pour l'IPA Kaggle dans un endroit temporaire sur votre poste de travail.\n",
        "\n",
        "<img style=\"margin-left:40px;\" src=\"https://courses.edx.org/asset-v1:UMontrealX+Cegep-Matane-VIARENA+2T2024+type@asset+block@Kaggle_API-5.png\"/>\n",
        "\n",
        "7. Maintenant, transférez (téléversez) votre clé privée « kaggle.json » dans votre environnement Colab. L'arborescence des fichiers dans Colab se trouve à gauche de la page du Notebook Colab.\n",
        "\n",
        "La fenêtre de l'outil de fichiers de votre ordinateur s'ouvre alors. Allez chercher votre clé privée « kaggle.json » que vous avez sauvegardée sur votre  ordinateur.\n",
        "\n",
        "<img style=\"margin-left:40px;\" src=\"https://courses.edx.org/asset-v1:UMontrealX+Cegep-Matane-VIARENA+2T2024+type@asset+block@Colab_Importer_Fichier.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2741da3d",
      "metadata": {
        "id": "2741da3d"
      },
      "source": [
        "8. Créer à la racine un répertoire .kaggle et déplacez votre clé privée « kaggle.json » dans ce répertoire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e6f0887",
      "metadata": {
        "id": "9e6f0887"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle -all"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ece582",
      "metadata": {
        "id": "69ece582"
      },
      "source": [
        "9. Maintenant téléchargez le jeu de données « chants-de-bruants» avec la commande suivante:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5ca9307",
      "metadata": {
        "id": "c5ca9307"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d claudecoulombe/chants-de-bruants/ --unzip -p \"donnees/sons_bruants/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "965a95cc",
      "metadata": {
        "id": "965a95cc"
      },
      "source": [
        "Une fois les deux indicateurs ci-dessus à 100%, on réorganise les répertoires pour supprimer un niveau hiérarchique de répertoires inutile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a74e517f",
      "metadata": {
        "id": "a74e517f"
      },
      "outputs": [],
      "source": [
        "# Réorganisation des répertoires\n",
        "\n",
        "!mv donnees/sons_bruants/bruant_a_gorge_blanche/bruant_a_gorge_blanche/*.wav donnees/sons_bruants/bruant_a_gorge_blanche/\n",
        "!rm -R donnees/sons_bruants/bruant_a_gorge_blanche/bruant_a_gorge_blanche/\n",
        "!mv donnees/sons_bruants/bruant_chanteur/bruant_chanteur/*.wav donnees/sons_bruants/bruant_chanteur/\n",
        "!rm -R donnees/sons_bruants/bruant_chanteur/bruant_chanteur/\n",
        "!mv donnees/sons_bruants/bruant_des_pres/bruant_des_pres/*.wav donnees/sons_bruants/bruant_des_pres/\n",
        "!rm -R donnees/sons_bruants/bruant_des_pres/bruant_des_pres/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7daf2e15",
      "metadata": {
        "id": "7daf2e15"
      },
      "source": [
        "### Répartition des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc82ffec",
      "metadata": {
        "id": "cc82ffec"
      },
      "outputs": [],
      "source": [
        "chants = np.array(tf.io.gfile.listdir(str(repertoire_donnees)))\n",
        "chants = chants[chants != '.ipynb_checkpoints']\n",
        "chants = chants[chants != '.DS_Store']\n",
        "print('Chants:', chants)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91889fe6",
      "metadata": {
        "id": "91889fe6"
      },
      "source": [
        "#### Extraction et répartition aléatoire des clips audio dans une liste appelée \"liste_de_fichiers_audio\"\n",
        "\n",
        "Le résultat est un structure de données de type matricielle appelée Tensor dans le jargon de TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "752ad1a2",
      "metadata": {
        "id": "752ad1a2"
      },
      "outputs": [],
      "source": [
        "liste_de_fichiers_audio = tf.io.gfile.glob(str(repertoire_donnees) + '/*/*')\n",
        "liste_de_fichiers_audio = tf.random.shuffle(liste_de_fichiers_audio)\n",
        "nombre_de_fichiers_audio = len(liste_de_fichiers_audio)\n",
        "print('Nombre total de fichiers audio:', nombre_de_fichiers_audio)\n",
        "print('Nombre de fichiers audio par classes:',\n",
        "      len(tf.io.gfile.listdir(str(repertoire_donnees/chants[0]))))\n",
        "print('Exemple de fichier audio:', liste_de_fichiers_audio[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4157f1db",
      "metadata": {
        "id": "4157f1db"
      },
      "source": [
        "#### Répartition des données d'entraînement, de validation et de tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47593990",
      "metadata": {
        "id": "47593990"
      },
      "outputs": [],
      "source": [
        "# Ici la répartition est 80:10:10\n",
        "fichiers_entrainement = liste_de_fichiers_audio[:144]\n",
        "fichiers_validation = liste_de_fichiers_audio[144: 144 + 18]\n",
        "fichiers_test = liste_de_fichiers_audio[-18:]\n",
        "\n",
        "print(\"Taille du jeu de données d'entraînement:\", len(fichiers_entrainement))\n",
        "print('Taille du jeu de données de validation:', len(fichiers_validation))\n",
        "print('Taille du jeu de données de test:', len(fichiers_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aedee22e",
      "metadata": {
        "id": "aedee22e"
      },
      "outputs": [],
      "source": [
        "print(fichiers_entrainement[:5])\n",
        "print(fichiers_validation[:5])\n",
        "print(fichiers_test[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2469061",
      "metadata": {
        "id": "b2469061"
      },
      "source": [
        "### Lecture des fichiers audio et des étiquettes de classes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b170ca00",
      "metadata": {
        "id": "b170ca00"
      },
      "source": [
        "Le prétraitement des données pour créer des représentations en ondes sonores binaires compatibles avec TensorFlow et les étiquettes de classes correspondantes.\n",
        "\n",
        "Notez que:\n",
        "\n",
        "* Chaque fichier .wav contient des données sonores formant une série chronologique ou série temporelle (<i>time series</i>) avec une fréquence d'échantillonnge fixe par seconde.\n",
        "* Chaque échantillon représente l'amplitude du signal audio à un moment précis.\n",
        "* Le taux d'échantillonnage pour cet ensemble de données en mp3 est généralement de 44.1 kHz ou 44 100 Hz\n",
        "* La structure de données retournée par tf.audio.decode_wav est (échantillons, canaux), où canaux est 1 pour mono ou 2 pour stéréo. Le jeu de données contient surtout des enregistrements mono mais également des enegistrements stéréos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34dfba52",
      "metadata": {
        "id": "34dfba52"
      },
      "outputs": [],
      "source": [
        "un_exemple_de_fichier_audio_wav = tf.io.read_file(chemin_donnees+\"/bruant_chanteur/XC125704.wav\")\n",
        "test_audio, _ = tf.audio.decode_wav(contents=un_exemple_de_fichier_audio_wav)\n",
        "test_audio.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "756c0f03",
      "metadata": {
        "id": "756c0f03"
      },
      "source": [
        "## Prétraitement des données\n",
        "#### Création de fonctions utilitaires, .wav vers TensorFlow\n",
        "Définissons une fonction qui fait le prétraitement des fichiers audio .wav bruts de l'ensemble de données pour les transformer en des représentations audio compatibles avec TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d39222",
      "metadata": {
        "id": "e1d39222"
      },
      "outputs": [],
      "source": [
        "def decode_audio_wav(fichier_audio_wav):\n",
        "    # Décode des fichiers binaires audio en format .wav vers un format TensorFlow 32 bits normalisé\n",
        "    # dans l'intervalle [-1.0, 1.0]. Retourne une structure float32 audio et une fréquene d'échantillonnage\n",
        "    # Les fichiers .wav ont deux canaux\n",
        "    audio, _ = tf.audio.decode_wav(contents=fichier_audio_wav,desired_channels=1)\n",
        "    # Puisque que toutes les données ont un seul canal (mono), supprimez l'axe \"canal\" du tableau.\n",
        "    return tf.squeeze(audio, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "821ef7b2",
      "metadata": {
        "id": "821ef7b2"
      },
      "outputs": [],
      "source": [
        "decode_audio_wav(un_exemple_de_fichier_audio_wav)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b320fdb4",
      "metadata": {
        "id": "b320fdb4"
      },
      "source": [
        "Maintenant, définissons une fonction qui crée des étiquettes en se basant sur les noms des répertoires parents pour chaque fichier. Nous allons utiliser `tf.RaggedTensors` car les structures matricielles (tenseurs) ont des dimensions irrégulières.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f78f820",
      "metadata": {
        "id": "4f78f820"
      },
      "outputs": [],
      "source": [
        "def extraire_etiquette(chemin_fichier):\n",
        "    morceaux_chemin = tf.strings.split(input=chemin_fichier,sep=os.path.sep)\n",
        "    return morceaux_chemin[-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "337f2742",
      "metadata": {
        "id": "337f2742"
      },
      "outputs": [],
      "source": [
        "extraire_etiquette(chemin_donnees+\"/bruant_chanteur/XC125704.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1ace2d9",
      "metadata": {
        "id": "b1ace2d9"
      },
      "source": [
        "Enfin, définissons une fonction utilitaire `obtenir_onde_etiquette` qui rassemble les deux fonctions précédentes :\n",
        "\n",
        "* L'entrée est le chemin du fichier audio .wav\n",
        "* La sortie est un tuple contenant une représentation de l'onde sonore et son étiquette de classe. Donc une donnée prête pour un apprentissage supervisé."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc867297",
      "metadata": {
        "id": "cc867297"
      },
      "outputs": [],
      "source": [
        "def obtenir_onde_etiquette(chemin_fichier_wav):\n",
        "    etiquette = extraire_etiquette(chemin_fichier_wav)\n",
        "    fichier_audio_wav = tf.io.read_file(chemin_fichier_wav)\n",
        "    onde_sonore_tf = decode_audio_wav(fichier_audio_wav)\n",
        "    return onde_sonore_tf, etiquette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74b98565",
      "metadata": {
        "id": "74b98565"
      },
      "outputs": [],
      "source": [
        "obtenir_onde_etiquette(chemin_donnees+\"/bruant_chanteur/XC125704.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0439000",
      "metadata": {
        "id": "d0439000"
      },
      "source": [
        "#### Prétraitement des ensembles de données d'entraînement, de validation et de test\n",
        "\n",
        "Nous alllons appliquer nos fonctions de prétraitement à l'ensemble d'entraînement, de validation et de test pour extraire les paires (représentation audio, étiquettes audio). En fait nous allons créer des générateurs de données de type `Dataset`.\n",
        "\n",
        "Pour cela, nous allons utiliser `tf.data.Dataset` avec `Dataset.from_tensor_slices` et `Dataset.map`, en utilisant `obtenir_onde_etiquette` que nous avons défini précédemment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4436d8bc",
      "metadata": {
        "id": "4436d8bc"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "donnees_entrainement = tf.data.Dataset.from_tensor_slices(fichiers_entrainement)\n",
        "\n",
        "representations_ondes_sonores = donnees_entrainement.map(\n",
        "    map_func=obtenir_onde_etiquette,\n",
        "    num_parallel_calls=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32412033",
      "metadata": {
        "id": "32412033"
      },
      "outputs": [],
      "source": [
        "fichiers_entrainement = tf.io.gfile.glob(str(chemin_donnees) + '/*/*')\n",
        "\n",
        "print(\"5 premières données:\\n\",fichiers_entrainement[:5])\n",
        "fichiers_entrainement = tf.random.shuffle(fichiers_entrainement)\n",
        "\n",
        "print(\"\\n5 premières données après mélange aléatoire:\\n\",fichiers_entrainement[:5])\n",
        "donnees_entrainement = tf.data.Dataset.from_tensor_slices(fichiers_entrainement)\n",
        "\n",
        "representations_ondes_sonores = donnees_entrainement.map(\n",
        "    map_func=obtenir_onde_etiquette,\n",
        "    num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "print(\"\\nType representations_ondes_sonores:\",type(representations_ondes_sonores))\n",
        "print(\"Taille representations_ondes_sonores:\",len(representations_ondes_sonores))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95a3983",
      "metadata": {
        "id": "d95a3983"
      },
      "source": [
        "### Affichage d'échantillons de représentations sous la forme d'ondes sonores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5cc7ea",
      "metadata": {
        "id": "de5cc7ea"
      },
      "outputs": [],
      "source": [
        "nombre_rangees = 3\n",
        "nombre_colonnes = 3\n",
        "nombre_donnees = nombre_rangees * nombre_colonnes\n",
        "fig, axes = plt.subplots(nombre_rangees, nombre_colonnes, figsize=(1.62*14, 14))\n",
        "\n",
        "for index_donnees, (onde_sonore, etiquette) in enumerate(representations_ondes_sonores.take(nombre_donnees)):\n",
        "    rangee_courante = index_donnees // nombre_colonnes\n",
        "    colonne_courante = index_donnees % nombre_colonnes\n",
        "    ax = axes[rangee_courante][colonne_courante]\n",
        "    ax.plot(onde_sonore.numpy())\n",
        "    ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "    etiquette = etiquette.numpy().decode('utf-8')\n",
        "    ax.set_title(etiquette)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1f6f59c",
      "metadata": {
        "id": "a1f6f59c"
      },
      "source": [
        "### Conversion des ondes en sonogrammes\n",
        "\n",
        "Les représentations sous la forme d'ondes de notre jeu de données sont dans le domaine temporel. Elles montrent l'amplitude du signal en fonction du temps. Pour mieux les analyser sous forme d'images 2D avec des réseaux convolutifs, nous allons transformer ces ondes en spectrogrammes ou sonogrammes passant de signaux du domaine temporel en signaux du domaine temps-fréquence grâce à la <a href=\"https://fr.wikipedia.org/wiki/Transform%C3%A9e_de_Fourier_%C3%A0_court_terme\" target='_blank'>transformée de Fourier locale</a> ou transformée de Fourier à fenêtre glissante, en anglais <i>Short Time Fourier Transform</i>, (<i>STFT</i>), pour convertir les formes d'onde en spectrogrammes, qui montrent les changements de fréquence au fil du temps et peuvent être représentés sous forme d'images 2D. Vous fournirez des images de sonogrammes en entrée à un réseau convolutif pour entraîner un modèle.\n",
        "\n",
        "Une transformée de Fourier (`tf.signal.fft`) convertit un signal en ses fréquences composantes, mais perd toutes les informations temporelles. En comparaison, la transformée de Fourier à fenêtre glissante (`tf.signal.stft`) divise le signal en fenêtres de temps et exécute une transformée de Fourier sur chaque fenêtre, en préservant certaines informations temporelles et en renvoyant un tenseur 2D sur lequel on peut effectuer des convolutions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32102045",
      "metadata": {
        "id": "32102045"
      },
      "source": [
        "#### Création d'une fonction utilitaire pour convertir les ondes sonores en sonogrammes\n",
        "\n",
        "Les ondes doivent avoir la même longueur, de sorte que lorsqu'on les convertit en spectrogrammes, les résultats aient des dimensions similaires. Cela peut être fait en complétant avec des zéros les clips audio qui durent moins d'une seconde (en utilisant `tf.zeros`).\n",
        "\n",
        "Lors de l'appel de `tf.signal.stft`, choisissez les paramètres `frame_length` et `frame_step` de sorte que \"l'image\" du spectrogramme généré soit presque carrée. Pour plus d'informations sur le choix des paramètres de `tf.signal.stft`, reportez-vous à cette <a href=\"https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe\" target='_blank'>vidéo de Coursera en anglais</a> sur le traitement du signal audio.\n",
        "\n",
        "La transformée de Fourier à fenêtre glissante produit un tableau de nombres complexes représentant l'amplitude et la phase. Cependant, dans ce laboratoire, vous n'utiliserez que l'amplitude, que vous pouvez dériver en appliquant `tf.abs` sur la sortie de `tf.signal.stft`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5c7de1",
      "metadata": {
        "id": "7f5c7de1"
      },
      "outputs": [],
      "source": [
        "def generer_sonogramme(onde_sonore):\n",
        "    # Compléter ou remplir avec des zéros si l'aonde sonore a moins de 41000 échantillon\n",
        "    longueur_entree = 41000\n",
        "    onde_sonore = onde_sonore[:longueur_entree]\n",
        "    remplissage_de_zeros = tf.zeros([41000] - tf.shape(onde_sonore),dtype=tf.float32)\n",
        "    # Convertir le type de l'onde sonore en un tenseur TensorFlow en float32\n",
        "    onde_sonore = tf.cast(onde_sonore, dtype=tf.float32)\n",
        "    # Concaténer l\"onde sonore avec le `remplissage_de_zeros`, ce qui garantit que\n",
        "     # tous les clips audio ont la même longueur.\n",
        "    onde_sonore_longueur_normalisee = tf.concat([onde_sonore, remplissage_de_zeros], 0)\n",
        "    # Convertir l'onde sonore en un sonogramme avec la fonction tf.signal.stft\n",
        "    # i.e. avec une transformée de Fourier à fenêtre glissante\n",
        "    sonogramme = tf.signal.stft(onde_sonore_longueur_normalisee, frame_length=255, frame_step=128)\n",
        "    # Obtenir la magnitude\n",
        "    sonogramme = tf.abs(sonogramme)\n",
        "    # Ajouter une dimension `canaux`, afin que le sonogramme puisse être utilisé\n",
        "    # en tant que données d'entrée de type image avec un réseau convolutif qui accepte\n",
        "    # en entrée un matrice de dimensions (`taille_lot`, `hauteur`, `largeur`, `canaux`)..\n",
        "    sonogramme = sonogramme[..., tf.newaxis]\n",
        "    return sonogramme"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4edf31f",
      "metadata": {
        "id": "c4edf31f"
      },
      "source": [
        "### Affichage d'échantillons de représentations sous forme de sonogrammes\n",
        "\n",
        "Explorons les données. Affichons l'onde sonore en représentation TensorFlow d'un exemple et son sonogramme correspondant, et écoutons le fichier audio d'origine :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9074e0dd",
      "metadata": {
        "id": "9074e0dd"
      },
      "outputs": [],
      "source": [
        "for onde_sonore_tf, etiquette in representations_ondes_sonores.take(2):\n",
        "    etiquette = etiquette.numpy().decode('utf-8')\n",
        "    sonogramme = generer_sonogramme(onde_sonore_tf)\n",
        "\n",
        "print(\"Étiquette:\", etiquette)\n",
        "print(\"Dimensions de la représentation onde sonore:\", onde_sonore_tf.shape)\n",
        "print(\"Dimensions de la représentation sonogramme:\", sonogramme.shape)\n",
        "print('Écoute du fichier audio original')\n",
        "display.display(display.Audio(onde_sonore_tf, rate=41000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3d2301d",
      "metadata": {
        "id": "a3d2301d"
      },
      "source": [
        "Maintenant, définissons une fonction pour afficher un sonogramme :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e5bd9a",
      "metadata": {
        "id": "17e5bd9a"
      },
      "outputs": [],
      "source": [
        "def afficher_sonogramme(sonogramme, ax):\n",
        "    if len(sonogramme.shape) > 2:\n",
        "        assert len(sonogramme.shape) == 3\n",
        "        sonogramme = np.squeeze(sonogramme, axis=-1)\n",
        "    # Convert the frequencies to log scale and transpose, so that the time is\n",
        "    # represented on the x-axis (columns).\n",
        "    # Add an epsilon to avoid taking a log of zero.\n",
        "    # Convertissez les fréquences sur une échelle logarithmique et transposez,\n",
        "    # de sorte que le temps soit représenté sur l'axe des abscisses (colonnes).\n",
        "     # Ajoutez un epsilon pour éviter de calculer le logarithmique de zéro.\n",
        "    log_sonogramme = np.log(sonogramme.T + np.finfo(float).eps)\n",
        "    hauteur = log_sonogramme.shape[0]\n",
        "    largeur = log_sonogramme.shape[1]\n",
        "    X = np.linspace(0, np.size(sonogramme), num=largeur, dtype=int)\n",
        "    Y = range(hauteur)\n",
        "    ax.pcolormesh(X, Y, log_sonogramme, shading='auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dd5df8a",
      "metadata": {
        "id": "7dd5df8a"
      },
      "source": [
        "Affichons la représentation de l'onde sonore dans le temps et le spectrogramme correspondant (fréquences dans le temps) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aefb646",
      "metadata": {
        "id": "2aefb646"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, figsize=(1.62*8, 8))\n",
        "timescale = np.arange(onde_sonore_tf.shape[0])\n",
        "axes[0].plot(timescale, onde_sonore_tf.numpy())\n",
        "axes[0].set_title('Onde sonore')\n",
        "axes[0].set_xlim([0, 41000])\n",
        "\n",
        "afficher_sonogramme(sonogramme.numpy(), axes[1])\n",
        "axes[1].set_title('Sonogramme')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b0bacb",
      "metadata": {
        "id": "d4b0bacb"
      },
      "source": [
        "Maintenant, définissons une fonction qui transforme les jeux de données contenant des ondes sonores en sonogrammes et leurs étiquettes correspondantes en ID entiers :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6544b35b",
      "metadata": {
        "id": "6544b35b"
      },
      "outputs": [],
      "source": [
        "def obtenir_sonogramme_etiquetteID(onde_sonore, etiquette):\n",
        "    sonogramme = generer_sonogramme(onde_sonore)\n",
        "    etiquette_id = tf.argmax(etiquette == chants)\n",
        "    return sonogramme, etiquette_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1044abb6",
      "metadata": {
        "id": "1044abb6"
      },
      "source": [
        "Appliquons `get_spectrogram_and_label_id` sur nos jeux de données avec la fonction `Dataset.map`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e4341e8",
      "metadata": {
        "id": "2e4341e8"
      },
      "outputs": [],
      "source": [
        "representations_sonogrammmes = representations_ondes_sonores.map(\n",
        "  map_func=obtenir_sonogramme_etiquetteID,\n",
        "  num_parallel_calls=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735767e",
      "metadata": {
        "id": "5735767e"
      },
      "source": [
        "### Affichage d'échantillons de représentations sous la forme de sonogrammes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8144a4d6",
      "metadata": {
        "id": "8144a4d6"
      },
      "outputs": [],
      "source": [
        "nombre_rangees = 3\n",
        "nombre_colonnes = 3\n",
        "nombre_donnees = nombre_rangees * nombre_colonnes\n",
        "fig, axes = plt.subplots(nombre_rangees, nombre_colonnes, figsize=(1.62*12, 12))\n",
        "\n",
        "for index_donnees, (sonogramme, etiquette) in enumerate(representations_sonogrammmes.take(nombre_donnees)):\n",
        "    rangee_courante = index_donnees // nombre_colonnes\n",
        "    colonne_courante = index_donnees % nombre_colonnes\n",
        "    ax = axes[rangee_courante][colonne_courante]\n",
        "    afficher_sonogramme(sonogramme.numpy(), ax)\n",
        "    ax.set_title(chants[etiquette.numpy()])\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cd05d99",
      "metadata": {
        "id": "3cd05d99"
      },
      "source": [
        "### Appliquer le prétraitement sur les jeux de données de validation et de test :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5a8aa75",
      "metadata": {
        "id": "c5a8aa75"
      },
      "outputs": [],
      "source": [
        "def pretraiter_jeux_donnees(chemin_fichier):\n",
        "    fichier_donnees = tf.data.Dataset.from_tensor_slices(chemin_fichier)\n",
        "    representations_sortie = fichier_donnees.map(\n",
        "        map_func=obtenir_onde_etiquette,\n",
        "        num_parallel_calls=AUTOTUNE)\n",
        "    representations_sortie = representations_sortie.map(\n",
        "        map_func=obtenir_sonogramme_etiquetteID,\n",
        "        num_parallel_calls=AUTOTUNE)\n",
        "    return representations_sortie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "865021bc",
      "metadata": {
        "id": "865021bc"
      },
      "outputs": [],
      "source": [
        "sonogrammes_entrainement = representations_sonogrammmes\n",
        "sonogrammes_validation = pretraiter_jeux_donnees(fichiers_validation)\n",
        "sonogrammes_test = pretraiter_jeux_donnees(fichiers_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a2ab89a",
      "metadata": {
        "id": "7a2ab89a"
      },
      "source": [
        "## Construction et entraînement d'un modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc0d6b7e",
      "metadata": {
        "id": "fc0d6b7e"
      },
      "source": [
        "### Découpage des données en lots pour l'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76f62c4",
      "metadata": {
        "id": "b76f62c4"
      },
      "outputs": [],
      "source": [
        "taille_lot = 64\n",
        "sonogrammes_entrainement = sonogrammes_entrainement.batch(taille_lot)\n",
        "sonogrammes_validation = sonogrammes_validation.batch(taille_lot)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "291d7e53",
      "metadata": {
        "id": "291d7e53"
      },
      "source": [
        "### Ajout des opérations de cache `Dataset.cache` et de pré-lecture `Dataset.prefetch`\n",
        "\n",
        "Afin de réduire la latence de lecture lors de l'entraînement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33c98392",
      "metadata": {
        "id": "33c98392"
      },
      "outputs": [],
      "source": [
        "sonogrammes_entrainement = sonogrammes_entrainement.cache().prefetch(AUTOTUNE)\n",
        "sonogrammes_validation = sonogrammes_validation.cache().prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bbbe0ae",
      "metadata": {
        "id": "0bbbe0ae"
      },
      "source": [
        "### Construction du modèle\n",
        "\n",
        "Pour le modèle, nous utiliserons un réseau convolutif, puisque nous avons transformé les fichiers audio en images de sonogrammes.\n",
        "\n",
        "Notre modèle `tf.keras.Sequential` utilisera les couches de prétraitement suivantes :\n",
        "\n",
        "* `tf.keras.layers.Resizing` : pour sous-échantillonner l'entrée afin de permettre au modèle de s'entraîner plus rapidement.\n",
        "* `tf.keras.layers.Normalization` : pour normaliser chaque pixel de l'image en fonction de sa moyenne et de son écart-type.\n",
        "\n",
        "Pour la couche de normalisation, la méthode `adapt` devrait être appelée sur les données d'entraînement afin de calculer des statistiques agrégées (c'est-à-dire la moyenne et l'écart type)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede69204",
      "metadata": {
        "id": "ede69204"
      },
      "outputs": [],
      "source": [
        "for sonogramme, _ in representations_sonogrammmes.take(1):\n",
        "    dimensions_entree = sonogramme.shape\n",
        "print('Dimensions Entree:', dimensions_entree)\n",
        "nombre_classes = len(chants)\n",
        "\n",
        "# Créer une couche de normalisation Keras\n",
        "couche_normalisation = layers.Normalization()\n",
        "# Ajuster la couche de normalisation au sonogramme avec la fonction `adapt`\n",
        "couche_normalisation.adapt(data=representations_sonogrammmes.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "modele = models.Sequential([\n",
        "    layers.Input(shape=dimensions_entree),\n",
        "    # Sous-échantillonner l'entrée\n",
        "    layers.Resizing(32, 32),\n",
        "    # Normalisation\n",
        "    couche_normalisation,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(nombre_classes),\n",
        "])\n",
        "\n",
        "#\n",
        "modele.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3afcc2e7",
      "metadata": {
        "id": "3afcc2e7"
      },
      "source": [
        "### Compilation du modèle\n",
        "\n",
        "Compilons le modèle Keras avec l'optimiseur `Adam` et la perte d'entropie croisée :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ee975c",
      "metadata": {
        "id": "c8ee975c"
      },
      "outputs": [],
      "source": [
        "modele.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a839ba33",
      "metadata": {
        "id": "a839ba33"
      },
      "source": [
        "### Entraînement du modèle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154cdc2a",
      "metadata": {
        "id": "154cdc2a"
      },
      "outputs": [],
      "source": [
        "iterations = 20\n",
        "traces_entrainement = modele.fit(\n",
        "    sonogrammes_entrainement,\n",
        "    validation_data=sonogrammes_validation,\n",
        "    epochs=iterations,\n",
        "    #callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=3),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f9f0efa",
      "metadata": {
        "id": "9f9f0efa"
      },
      "source": [
        "### Affichage des courbes d'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a779e03",
      "metadata": {
        "id": "5a779e03"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(1.62*6, 6))\n",
        "metriques = traces_entrainement.history\n",
        "plt.plot(traces_entrainement.epoch, metriques['loss'], metriques['val_loss'])\n",
        "plt.legend([\"erreur d'entraînement \", \"erreur de validation\"])\n",
        "plt.xlabel(\"Nombre d'itérations\")\n",
        "plt.ylabel(\"Erreur d'entropie\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fac68a1",
      "metadata": {
        "id": "8fac68a1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(1.62*6, 6))\n",
        "metriques = traces_entrainement.history\n",
        "plt.plot(traces_entrainement.epoch, metriques['accuracy'], metriques['val_accuracy'])\n",
        "plt.legend([\"exactitude d'entraînement\", 'exactitude de validation'])\n",
        "plt.xlabel(\"Nombre d'itérations\")\n",
        "plt.ylabel(\"Exactitude\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be69009",
      "metadata": {
        "id": "6be69009"
      },
      "source": [
        "## Évaluation du modèle\n",
        "\n",
        "Nous allons évaluer la performance du modèle sur les données de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c4d156",
      "metadata": {
        "id": "46c4d156"
      },
      "outputs": [],
      "source": [
        "test_audio = []\n",
        "test_etiquettes = []\n",
        "\n",
        "for audio, etiquette in sonogrammes_test:\n",
        "    test_audio.append(audio.numpy())\n",
        "    test_etiquettes.append(etiquette.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_etiquettes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ef55c6",
      "metadata": {
        "id": "34ef55c6"
      },
      "outputs": [],
      "source": [
        "classes_predites = np.argmax(modele.predict(test_audio), axis=1)\n",
        "vraies_classes = test_etiquettes\n",
        "\n",
        "exactitude_test = sum(classes_predites == vraies_classes) / len(vraies_classes)\n",
        "print(f'Exactitude sur les données de test: {exactitude_test:.0%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c32966ee",
      "metadata": {
        "id": "c32966ee"
      },
      "source": [
        "### Affichage d'une matrice de confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2735ac5",
      "metadata": {
        "id": "e2735ac5"
      },
      "outputs": [],
      "source": [
        "confusion_mtx = tf.math.confusion_matrix(vraies_classes, classes_predites)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx,\n",
        "            xticklabels=chants,\n",
        "            yticklabels=chants,\n",
        "            annot=True, fmt='g')\n",
        "plt.xlabel('Classes prédites')\n",
        "plt.ylabel('Vraies classes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbb66419",
      "metadata": {
        "id": "bbb66419"
      },
      "source": [
        "## Test en inférence sur des données fraîches\n",
        "\n",
        "Enfin, vérifiez la prédiction du modèle à l'aide d'un nouveau fichier audio pris sur Xeno-canto. Quelle est la performance de votre modèle ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d5ba523",
      "metadata": {
        "id": "6d5ba523"
      },
      "outputs": [],
      "source": [
        "!wget \"https://xeno-canto.org/550411/download\" -O \"550411-bruant_des_pres.mp3\"\n",
        "chemin_fichier_mp3 = \"550411-bruant_des_pres.mp3\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcd2c2e5",
      "metadata": {
        "id": "dcd2c2e5"
      },
      "source": [
        "### Conversion de .mp3 à .wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c005af58",
      "metadata": {
        "id": "c005af58"
      },
      "outputs": [],
      "source": [
        "# Installation bibliothèque de conversion .mp3 à .wav\n",
        "!pip3 install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f98cfa",
      "metadata": {
        "id": "d8f98cfa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import path\n",
        "from pydub import AudioSegment\n",
        "\n",
        "fichier_mp3 = AudioSegment.from_mp3(chemin_fichier_mp3)\n",
        "chemin_fichier_wav = \"donnees/sons_bruants/bruant_des_pres/Fichier_TEST\"\n",
        "fichier_mp3.export(chemin_fichier_wav,format=\"wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0a5c5a5",
      "metadata": {
        "id": "a0a5c5a5"
      },
      "outputs": [],
      "source": [
        "sonogrammes_inference = pretraiter_jeux_donnees([str(chemin_fichier_wav)])\n",
        "\n",
        "for sonogramme, etiquettte in sonogrammes_inference.batch(1):\n",
        "    classe_predite = modele(sonogramme)\n",
        "    plt.bar(chants, tf.nn.softmax(classe_predite[0]))\n",
        "    plt.title(f'Prediction: \"{chants[etiquettte[0]]}\"')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Exécution du carnet web IPython terminée\")"
      ],
      "metadata": {
        "id": "dvXs-Kxr5r90"
      },
      "id": "dvXs-Kxr5r90",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "name": "Distinction_Chants_Bruants.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}