{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dwj7vA5rwFJ"
      },
      "source": [
        "<a style=\"float:left;\" href=\"https://colab.research.google.com/github/ClaudeCoulombe/VIARENA/blob/master/Labos/Lab-CIFAR_10-reseau_autoattentif/Identification_Objets-CIFAR_10-reseau_autoattentif.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><br/>\n",
        "\n",
        "### Rappel - Fonctionnement d'un carnet web iPython\n",
        "\n",
        "* Pour exécuter le code contenu dans une cellule d'un carnet iPython, cliquez dans la cellule et faites (⇧↵, shift-enter);\n",
        "* Le code d'un carnet iPython s'exécute séquentiellement de haut en bas de la page. Souvent, l'importation d'une bibliothèque Python ou l'initialisation d'une variable est préalable à l'exécution d'une cellule située plus bas. Il est donc recommandé d'exécuter les cellules en séquence. Enfin, méfiez-vous des retours en arrière qui peuvent réinitialiser certaines variables;\n",
        "* Pour obtenir de l'information sur une fonction, utilisez la commande Python `help(`\"nom de la fonction\"`)`\n",
        "\n",
        "<b>SVP</b>, déployez toutes les cellules en sélectionnant l'item « Développer les rubriques » de l'onglet « Affichage »."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identification d'objets à partir de photos - jeu de données CIFAR-10\n",
        "\n",
        "# avec un réseau autoattentif pour la vision\n",
        "\n",
        "### Inspiration et droits d'auteur\n",
        "\n",
        "Ce laboratoire s'inspire de plusieurs oeuvres en logiciels libres qui ont été transformées.\n",
        "\n",
        "Il s'agit d'une implémentation un réseau autoattentif pour la vision, en anglais, Vision Transformer (ViT), basé sur l'article [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929/) par Alexey Dosovitskiy et al. pour la classification des images et le démontre sur l'ensemble de données CIFAR-100. Cette architecture autoattentive utilise des séquences de tuiles d'image, sans utiliser de couches de convolution.\n",
        "\n",
        "Plus précisément, ce laboratoire est une traduction et adaptation par [Claude Coulombe](https://www.linkedin.com/in/claudecoulombe/) du [carnet IPython **« Image classification with Vision Transformer »**](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/image_classification_with_vision_transformer.ipynb) de [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/) Description: Implementing the Vision Transformer (ViT) model for image classification.\n",
        "<br/><br/>\n",
        "##### Copyright (c) 2021, Alexey Dosovitskiy et al.\n",
        "##### Copyright (c) 2021, Khalid Salama\n",
        "##### Copyright (c) 2024, Claude Coulombe"
      ],
      "metadata": {
        "id": "mekgOGhUU3bf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENKSo4rArwFK"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade keras\n",
        "!pip install keras==3.3.3\n",
        "print()\n",
        "print(\"Vous devez redémarrer la session\")"
      ],
      "metadata": {
        "id": "0MDho0x1smPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncYHMIB7rwFL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"tensorFlow version:\",tf.__version__)\n",
        "print(\"keras version:\",keras.__version__)\n",
        "print(\"matplotlib:\",matplotlib.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydjLNGx6rwFL"
      },
      "source": [
        "## Prétraitement des données\n",
        "\n",
        "### Jeu de données - photos CIFAR-10\n",
        "L'ensemble de données CIFAR-10 (Canadian Institute For Advanced Research) comporte 60 000 photographies en couleur de 32×32 pixels d'objets de 10 classes différentes. Il est relativement simple d'atteindre une précision de 80 %. On peut obtenir des performances de 90 % pour ces données avec des réseaux neuronaux convolutifs.\n",
        "\n",
        "* 0 : avion\n",
        "* 1 : automobile\n",
        "* 2 : oiseau\n",
        "* 3 : chat\n",
        "* 4 : cerf\n",
        "* 5 : chien\n",
        "* 6 : grenouille\n",
        "* 7 : cheval\n",
        "* 8 : bateau\n",
        "* 9 : camion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# le jeu de données CIFAR-10\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "dic_noms_etiquette = {\n",
        "    0 : \"avion\",\n",
        "    1 : \"automobile\",\n",
        "    2 : \"oiseau\",\n",
        "    3 : \"chat\",\n",
        "    4 : \"cerf\",\n",
        "    5 : \"chien\",\n",
        "    6 : \"grenouille\",\n",
        "    7 : \"cheval\",\n",
        "    8 : \"bateau\",\n",
        "    9 : \"camion\",\n",
        "}\n",
        "\n",
        "nombre_classes = 10\n",
        "dim_entree = (32, 32, 3)\n",
        "\n",
        "# lire le jeu de données CIFAR-10 et le diviser entre\n",
        "# les données d'entrainement et les données de test\n",
        "(attributs_entrainement, classes_cibles_entrainement), (attributs_test, classes_cibles_test) = cifar10.load_data()\n",
        "\n",
        "# Aperçu des tableaux de données\n",
        "print()\n",
        "print('Entraînement: attributs=%s, classes=%s' % (attributs_entrainement.shape, classes_cibles_entrainement.shape))\n",
        "print('Test: attributs=%s, classes=%s' % (attributs_test.shape, classes_cibles_test.shape))\n",
        "print()\n",
        "print(f\"attributs_entrainement dim: {attributs_entrainement.shape} - y_train shape: {classes_cibles_entrainement.shape}\")\n",
        "print(f\"attributs_test shape: {attributs_test.shape} - classes_cibles_test shape: {classes_cibles_test.shape}\")\n",
        "\n",
        "# Afficher les 36 premières images\n",
        "print()\n",
        "print(\"Quelques images avec leur étiquette...\")\n",
        "%matplotlib inline\n",
        "# définir subplot\n",
        "#fig, axes = plt.subplots(nrows=6,ncols=6,figsize=(10,8))\n",
        "\n",
        "fig, axes = plt.subplots(nrows=6,ncols=6,figsize=(10,12))\n",
        "# for i_rangee in range(0,4):\n",
        "for i_rangee in range(0,6):\n",
        "    for i_colonne in range(0,6):\n",
        "        axes[i_rangee,i_colonne].set_title(dic_noms_etiquette[int(classes_cibles_entrainement[i_rangee*6+i_colonne])],\n",
        "                                          fontsize=10, color=\"#FF0000\")\n",
        "        # axes[i_rangee,i_colonne].set_title(int(classes_cibles_entrainement[i_rangee*6+i_colonne]),\n",
        "        #                                    fontsize=10, color=\"#FF0000\")\n",
        "        axes[i_rangee,i_colonne].imshow(attributs_entrainement[i_rangee*6+i_colonne])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A5tD_EQPfHrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8ZnwUkqrwFL"
      },
      "source": [
        "## Configuration des hyperparamètres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML6T1jevrwFL"
      },
      "outputs": [],
      "source": [
        "taux_apprentissage = 0.001\n",
        "taux_decroissance_poids = 0.0001\n",
        "\n",
        "taille_lots = 256\n",
        "\n",
        "nombre_epoques = 100 # utilisez 10 pour vos premiers tests, mais 100 pour\n",
        "\n",
        "taille_image = 72 # Nous allons redimensionner les images à cette taille\n",
        "\n",
        "taille_tuile = 6  # Taille des tuiles à extraire des images d'entrée\n",
        "\n",
        "nombre_tuiles = (taille_image // taille_tuile) ** 2\n",
        "\n",
        "taille_vecteur_semantique = 64\n",
        "nombre_tetes = 4\n",
        "\n",
        "unite_autoattentive = [\n",
        "    taille_vecteur_semantique * 2,\n",
        "    taille_vecteur_semantique,\n",
        "]  # taille des couches autoattentives\n",
        "\n",
        "nombre_couches_autoattentives = 8\n",
        "\n",
        "unite_perceptron =  [\n",
        "    2048,\n",
        "    1024,\n",
        "]  # Dimension des couches denses du classificateur final\n",
        "\n",
        "print(\"Hyperparamètres initialisés\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGHcEQBbrwFM"
      },
      "source": [
        "## Couche Amplification des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1cgojgbrwFM"
      },
      "outputs": [],
      "source": [
        "amplification_donnees = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(taille_image, taille_image),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "    ],\n",
        "    name=\"amplification_donnees\",\n",
        ")\n",
        "\n",
        "# Calcul de la moyenne et de la variance des données d'entraînement pour la normalisation.\n",
        "amplification_donnees.layers[0].adapt(attributs_entrainement)\n",
        "\n",
        "print(\"Préparation de la couche d'amplification des données\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGZO6HDgrwFM"
      },
      "source": [
        "## Couche perceptron multicouche (PMC)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOXZITMyrwFN"
      },
      "outputs": [],
      "source": [
        "def perceptron_multicouche(x, unites_cachees, taux_extinction):\n",
        "    for unite in unites_cachees:\n",
        "        x = layers.Dense(unite, activation=keras.activations.gelu)(x)\n",
        "        x = layers.Dropout(taux_extinction)(x)\n",
        "    return x\n",
        "\n",
        "print(\"Code de création d'un perceptron prêt!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIRulhQerwFN"
      },
      "source": [
        "## Couche de création de tuiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGlxVH51rwFN"
      },
      "outputs": [],
      "source": [
        "class Tuiles(layers.Layer):\n",
        "    def __init__(self, taille_tuile):\n",
        "        super().__init__()\n",
        "        self.taille_tuile = taille_tuile\n",
        "\n",
        "    def call(self, images):\n",
        "        dim_entree = ops.shape(images)\n",
        "        taille_lots = dim_entree[0]\n",
        "        hauteur = dim_entree[1]\n",
        "        largeur = dim_entree[2]\n",
        "        nombre_canaux = dim_entree[3]\n",
        "        nombre_tuiles_hauteur = hauteur // self.taille_tuile\n",
        "        nombre_tuiles_largeur = largeur // self.taille_tuile\n",
        "        tuiles = keras.ops.image.extract_patches(images, size=self.taille_tuile)\n",
        "        tuiles = ops.reshape(\n",
        "            tuiles,\n",
        "            (\n",
        "                taille_lots,\n",
        "                nombre_tuiles_hauteur * nombre_tuiles_largeur,\n",
        "                self.taille_tuile * self.taille_tuile * nombre_canaux,\n",
        "            ),\n",
        "        )\n",
        "        return tuiles\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"taille_tuile\": self.taille_tuile})\n",
        "        return config\n",
        "\n",
        "print(\"Code classe Tuiles prêt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxN-agy8rwFN"
      },
      "source": [
        "Affichons les tuiles créées à partir d'un exemple d'image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpJkXxwtrwFN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(4, 4))\n",
        "image = attributs_entrainement[np.random.choice(range(attributs_entrainement.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_redimensionnee = ops.image.resize( ops.convert_to_tensor([image]), size=(taille_image, taille_image))\n",
        "\n",
        "tuiles = Tuiles(taille_tuile)(image_redimensionnee)\n",
        "print(f\"Taille image: {taille_image} X {taille_image}\")\n",
        "print(f\"Taille tuile: {taille_tuile} X {taille_tuile}\")\n",
        "print(f\"Nombre de tuiles par image: {tuiles.shape[1]}\")\n",
        "print(f\"Éléments par tuile: {tuiles.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(tuiles.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, tuile in enumerate(tuiles[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    tuile_image = ops.reshape(tuile, (taille_tuile, taille_tuile, 3))\n",
        "    plt.imshow(ops.convert_to_numpy(tuile_image).astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "bCHQW1lvITh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-y9wfBRrwFN"
      },
      "source": [
        "##  Couche d'encodage de tuiles\n",
        "\n",
        "La couche `EncodeurTuiles` transforme linéairement une tuile en la projetant dans un vecteur sémantique de taille `taille_vecteur_semantique`. De plus, il ajoute une position apprise par entraînement au vecteur sémantique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1soOtuarwFN"
      },
      "outputs": [],
      "source": [
        "class EncodeurTuiles(layers.Layer):\n",
        "    def __init__(self, nombre_tuiles, taille_vecteur_semantique):\n",
        "        super().__init__()\n",
        "        self.nombre_tuiles = nombre_tuiles\n",
        "        self.projection = layers.Dense(units=taille_vecteur_semantique)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=nombre_tuiles, output_dim=taille_vecteur_semantique\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = ops.expand_dims(\n",
        "            ops.arange(start=0, stop=self.nombre_tuiles, step=1), axis=0\n",
        "        )\n",
        "        tuiles_enrichies = self.projection(patch)\n",
        "        tuiles_encodees = tuiles_enrichies + self.position_embedding(positions)\n",
        "        return tuiles_encodees\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"nombre_tuiles\": self.nombre_tuiles})\n",
        "        return config\n",
        "\n",
        "print(\"EncodeurTuiles prêt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGHrYsHurwFN"
      },
      "source": [
        "## Construire le réseau autoattenfif (ViT)\n",
        "\n",
        "Le réseau de vision autoattenfif (ViT) se compose de plusieurs blocs autoattentif (Transformer), qui utilisent la couche `layers.MultiHeadAttention` comme mécanisme d'auto-attention appliqué à la séquence de tuiles. Les blocs autoattentifs produisent un Tenseur `[taille_lots, nombre_tuiles, taille_vecteur_semantique]`, qui est traité via un tête de classificateur avec softmax pour produire la sortie finale des probabilités pour chaque classe.\n",
        "<br/><br/>\n",
        "Contrairement à la technique décrite dans le [article](https://arxiv.org/abs/2010.11929), qui ajoute un vecteur sémantique appris par entraînement à la séquence de tuiles encodées pour servir de représentation de l'image, toutes les sorties du bloc autoattentif final sont remodelées avec une couche d'aplatissement `layers.Flatten()` et utilisées comme représentation de l'image comme entrée dans la tête du classificateur.\n",
        "Notez que la couche `layers.GlobalAveragePooling1D` pourrait également être utilisée à la place pour agréger les sorties du bloc autoattentif,\n",
        "surtout lorsque le nombre de tuiles et les dimensions du vecteur sémantque sont importants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FQkJviWrwFN"
      },
      "outputs": [],
      "source": [
        "def classificateur_autoattentif():\n",
        "    entrees = keras.Input(shape=dim_entree)\n",
        "    # Amplifier les données\n",
        "    donnnees_augmentees = amplification_donnees(entrees)\n",
        "    # Créer des tuiles\n",
        "    tuiles = Tuiles(taille_tuile)(donnnees_augmentees)\n",
        "    # Encoder les tuiles\n",
        "    tuiles_encodees = EncodeurTuiles(nombre_tuiles, taille_vecteur_semantique)(tuiles)\n",
        "    # Créer plusieurs couches autoattentives\n",
        "    for _ in range(nombre_couches_autoattentives):\n",
        "        # couche de normalisation 1\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(tuiles_encodees)\n",
        "        # Créer une couche autoattentive multi-têtes\n",
        "        sortie_couche_attentive = layers.MultiHeadAttention(\n",
        "            num_heads=nombre_tetes, key_dim=taille_vecteur_semantique, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # connexion résiduelle 1\n",
        "        x2 = layers.Add()([sortie_couche_attentive, tuiles_encodees])\n",
        "        # couche de normalisation 1.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # couche perceptron multicouche\n",
        "        x3 = perceptron_multicouche(x3, unites_cachees=unite_autoattentive, taux_extinction=0.1)\n",
        "        # connexion résiduelle 2\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Crer un tenseur a [taille_lots, taille_vecteur_semantique]\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(tuiles_encodees)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Ajouter le perceptron multicouche de sortie\n",
        "    attributs = perceptron_multicouche(representation, unites_cachees=unite_perceptron, taux_extinction=0.5)\n",
        "    # Classifier les sorties\n",
        "    logits = layers.Dense(nombre_classes)(attributs)\n",
        "    # Créer le modèle Keras\n",
        "    modele = keras.Model(inputs=entrees, outputs=logits)\n",
        "    return modele\n",
        "\n",
        "print(\"Classificateur_autoattentif prêt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axi6r-durwFN"
      },
      "source": [
        "## Entraîner, et évaluer le modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enIyrDZ8rwFN"
      },
      "outputs": [],
      "source": [
        "def entrainer_modele(model):\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=taux_apprentissage, weight_decay=taux_decroissance_poids\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    traces = model.fit(\n",
        "        x=attributs_entrainement,\n",
        "        y=classes_cibles_entrainement,\n",
        "        batch_size=taille_lots,\n",
        "        epochs=nombre_epoques,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(attributs_test, classes_cibles_test)\n",
        "    print(f\"Exactitude sur les données de test: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Exactitude sur la prévision des 5 meilleurs: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return traces\n",
        "\n",
        "modele_autoattentif = classificateur_autoattentif()\n",
        "traces = entrainer_modele(modele_autoattentif)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mini_dict = { \"erreur d'entropie\":\"loss\",\n",
        "              \"exactitude\":\"accuracy\",\n",
        "              \"exactitude 5 meilleurs\":\"top-5-accuracy\"\n",
        "}\n",
        "\n",
        "def afficher_traces(item):\n",
        "    plt.plot(traces.history[mini_dict[item]], label=item)\n",
        "    plt.plot(traces.history[\"val_\" + mini_dict[item]], label=\"val_\" + item)\n",
        "    plt.xlabel(\"Nombre d'époques\")\n",
        "    plt.ylabel(item)\n",
        "    plt.title(\"Courbes d'entraînement et de validation pour {}\".format(item), fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "afficher_traces(\"exactitude\")\n",
        "afficher_traces(\"exactitude 5 meilleurs\")\n",
        "afficher_traces(\"erreur d'entropie\")\n"
      ],
      "metadata": {
        "id": "MGRREaonm8O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MpCRImDrwFO"
      },
      "source": [
        "Après 100 époques, notre réseau autoattentif atteint une exactitude d'environ 70 % sur les données de test et 97 % sur la prévision des cinq meilleures classes. Il ne s'agit pas de résultats très impressionnants sur l'ensemble de données CIFAR-10, car un réseau convolutif ResNet50V2, entraîné à partir de zéro sur les mêmes données, peut atteindre une exactitude de plus de 90 %.\n",
        "\n",
        "Il convient de noter que les résultats de l'état de l'art rapportés dans l'[article](https://arxiv.org/abs/2010.11929) sont obtenus en pré-entraînant le réseau autoattentif sur le jeu de données JFT-300M, puis l'affiner sur l'ensemble de données appplicatif. Pour améliorer la qualité du modèle sans pré-entraînement, vous pouvez essayer d'entraîner le modèle pendant plus d'époques, utiliser un plus grand nombre de couches autoattentives, redimensionner les images d'entrée, modifier la taille des tuiles ou augmenter la dimension du vecteur sémantique, etc.\n",
        "\n",
        "En outre, comme mentionné dans l'article, la qualité du modèle n'est pas seulement affectée par le choix d'architecture, mais aussi par des paramètres tels que l'évolution du taux d'apprentissage, l'optimiseur, le taux de décroissance des poids, etc.\n",
        "\n",
        "En pratique, il est recommandé d'affiner un modèle autoattentif qui a été pré-entraîné à l’aide d’un vaste ensemble de photos à haute résolution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Exécution du carnet IPython terminée\")"
      ],
      "metadata": {
        "id": "ogYvC053_TLN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}