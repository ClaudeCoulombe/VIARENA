{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVQ7Ein3FCZG"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaudeCoulombe/VIARENA/blob/master/Labos/Lab-Identification_Arbres/Id_arbre-reseau_convolutif-transfert_amplification-colab.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "### Rappel - Fonctionnement d'un carnet web iPython\n",
        "\n",
        "* Pour exécuter le code contenu dans une cellule d'un carnet iPython, cliquez dans la cellule et faites (⇧↵, shift-enter) \n",
        "* Le code d'un carnet iPython s'exécute séquentiellement de haut en bas de la page. Souvent, l'importation d'une bibliothèque Python ou l'initialisation d'une variable est préalable à l'exécution d'une cellule située plus bas. Il est donc recommandé d'exécuter les cellules en séquence. Enfin, méfiez-vous des retours en arrière qui peuvent réinitialiser certaines variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "# Identification d'arbres à partir de leur écorce\n",
        "## Réseau convolutif, apprentissage par transfert et amplification des données\n",
        "\n",
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZSlp3DAjdYf"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq38SFqsf7b0"
      },
      "source": [
        "# Apprentissage par transfert & amplification des données"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixer le hasard pour la reproductibilité\n",
        "\n",
        "La mise au point de réseaux de neurones implique certains processus aléatoires. Afin de pouvoir reproduire et comparer vos résultats d'expérience, vous fixez temporairement l'état aléatoire grâce à un germe aléatoire unique.\n",
        "\n",
        "Pendant la mise au point, vous fixez temporairement l'état aléatoire pour la reproductibilité mais vous répétez l'expérience avec différents germes ou états aléatoires et prenez la moyenne des résultats.\n",
        "<br/>\n",
        "**Note** : Pour un système en production, vous ravivez simplement l'état  purement aléatoire avec l'instruction `GERME_ALEATOIRE = None`"
      ],
      "metadata": {
        "id": "Ssu3_pic0Xw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Définir un germe aléatoire\n",
        "GERME_ALEATOIRE = 1\n",
        "\n",
        "# Définir un état aléatoire pour Python\n",
        "os.environ['PYTHONHASHSEED'] = str(GERME_ALEATOIRE)\n",
        "\n",
        "# Définir un état aléatoire pour Python random\n",
        "import random\n",
        "random.seed(GERME_ALEATOIRE)\n",
        "\n",
        "# Définir un état aléatoire pour NumPy\n",
        "import numpy as np\n",
        "np.random.seed(GERME_ALEATOIRE)\n",
        "\n",
        "# Définir un état aléatoire pour TensorFlow\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(GERME_ALEATOIRE)\n",
        "\n",
        "# Retrait du comportement déterministe\n",
        "# à cause de keras.layers.RandomContrast(...)\n",
        "# dont il n'existe pas de version déterministe\n",
        "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "# os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "print(\"Germe aléatoire fixé\")"
      ],
      "metadata": {
        "id": "lh5IxjSf0Wdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acquisition des donnés"
      ],
      "metadata": {
        "id": "DZuDHKSh0gtB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snh2vdTCTBhy"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax2GDFiCCJNt"
      },
      "outputs": [],
      "source": [
        "# Note that we did not use Acer platanoides (2), Pinus rigida (15) and Populus grandidentata (18)\n",
        "# since we did not collect enough images in these categories to obtain meaningful results.\n",
        "data_ecorces = {\n",
        "    'SAB': 1,  \n",
        "#    'ERB': 2,  # Pas assez de spécimens - seulement 1\n",
        "    'ERR': 3, \n",
        "    'ERS': 4, \n",
        "    'BOJ': 5, \n",
        "    'BOP': 6,\n",
        "    'HEG': 7,  \n",
        "    'FRA': 8, \n",
        "    'MEL': 9,  \n",
        "    'OSV': 10, \n",
        "    'EPO': 11,\n",
        "    'EPB': 12,\n",
        "    'EPN': 13,\n",
        "    'EPR': 14,\n",
        "#    'PID': 15, # Pas assez de spécimens - seulement 4\n",
        "    'PIR': 16, \n",
        "    'PIB': 17, \n",
        "#    'PEG': 18, # Pas assez de spécimens - seulement 3\n",
        "    'PET': 19, \n",
        "    'CHR': 20,\n",
        "    'THO': 21, \n",
        "    'PRU': 22, \n",
        "    'ORA': 23  \n",
        "}\n",
        "\n",
        "noms_arbres = {\n",
        "            1: '\\emph{Abies balsamea} - Sapin Baumier - Balsam fir',\n",
        "            2: '\\emph{Acer platanoides} - Érable de Norvège - Norway maple',\n",
        "            3: '\\emph{Acer rubrum} - Érable rouge - Red maple',\n",
        "            4: '\\emph{Acer saccharum} - Érable à sucre - Sugar maple',\n",
        "            5: '\\emph{Betula alleghaniensis} - Bouleau jaune - Yellow birch',\n",
        "            6: '\\emph{Betula papyrifera} - Bouleau à papier - White birch',\n",
        "            7: '\\emph{Fagus grandifolia} - Hêtre à grandes feuilles - American beech',\n",
        "            8: \"\\emph{Fraxinus americana} - Frêne d'Amérique - White ash\",\n",
        "            9: '\\emph{Larix laricina} - Mélèze - Tamarack',\n",
        "            10: '\\emph{Ostrya virginiana} - Ostryer de Virginie - American hophornbeam',\n",
        "            11: '\\emph{Picea abies} - Épinette de Norvège - Norway spruce',\n",
        "            12: '\\emph{Picea glauca} - Épinette blanche - White spruce',\n",
        "            13: '\\emph{Picea mariana} - Épinette noire - Black spruce',\n",
        "            14: '\\emph{Picea rubens} - Épinette rouge - Red spruce',\n",
        "            15: '\\emph{Pinus rigida} - Pin rigide - Pitch pine',\n",
        "            16: '\\emph{Pinus resinosa} - Pin rouge - Red pine',\n",
        "            17: '\\emph{Pinus strobus} - Pin blanc - Eastern white pine',\n",
        "            18: '\\emph{Populus grandidentata} - Peuplier à grandes dents - Big-tooth aspen',\n",
        "            19: '\\emph{Populus tremuloides} - Peuplier faux tremble - Quaking aspen',\n",
        "            20: '\\emph{Quercus rubra} - Chêne rouge - Northern red oak',\n",
        "            21: '\\emph{Thuja occidentalis} - Thuya occidental - Northern white cedar',\n",
        "            22: '\\emph{Tsuga canadensis} - Pruche du Canada - Eastern Hemlock',\n",
        "            23: \"\\emph{Ulmus americana} - Orme d'Amérique - American elm\"\n",
        "        }\n",
        "\n",
        "dict_no_arbres_ID = {\n",
        "    '1':'SAB',  \n",
        "    '2':'ERB',\n",
        "    '3':'ERR', \n",
        "    '4':'ERS', \n",
        "    '5':'BOJ', \n",
        "    '6':'BOP',\n",
        "    '7':'HEG',  \n",
        "    '8':'FRA', \n",
        "    '9':'MEL',  \n",
        "    '10':'OSV', \n",
        "    '11':'EPO',\n",
        "    '12':'EPB',\n",
        "    '13':'EPN',\n",
        "    '14':'EPR',\n",
        "    '15':'PID',\n",
        "    '16':'PIR', \n",
        "    '17':'PIB',\n",
        "    '18':'PEG',\n",
        "    '19':'PET', \n",
        "    '20':'CHR',\n",
        "    '21':'THO', \n",
        "    '22':'PRU', \n",
        "    '23':'ORA'  \n",
        "}\n",
        "\n",
        "print(\"Code exécuté\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8ADcq2QPbu6"
      },
      "outputs": [],
      "source": [
        "data_zip_urls_dict = {\n",
        "   \"BOJ\":\"https://drive.google.com/file/d/1d2zxg2pt5S8UJIK-E7IuWfGN0d1kxxMw/view?usp=sharing\",\n",
        "   \"BOP\":\"https://drive.google.com/file/d/12cg6UO4HLnjk5fE_KXtrgdC2s8uGh4Zp/view?usp=sharing\",\n",
        "   \"CHR\":\"https://drive.google.com/file/d/1Nq19-I-Q577KXMTFrkhlJDhMfclh0cWn/view?usp=sharing\",\n",
        "   \"EPB\":\"https://drive.google.com/file/d/1K_Ncw8VEiuDZ_iJDbYToMq-GO5dzKHns/view?usp=sharing\",\n",
        "   \"EPN\":\"https://drive.google.com/file/d/1S309DYmg76SrIA89aVQWXCMwm6CzhN8b/view?usp=sharing\",\n",
        "   \"EPO\":\"https://drive.google.com/file/d/1fTKEcpYgmRg4spUpcH0FAiAnoRgANafL/view?usp=sharing\",\n",
        "   \"EPR\":\"https://drive.google.com/file/d/1qRhtZ8LZjH_45fxetG7swg3ok3znk8CJ/view?usp=sharing\",\n",
        "#   \"ERB\":\"https://drive.google.com/file/d/1ighbGniKAT_GrPm4RtsIAuN1STg9sjR9/view?usp=sharing\", # Assez de données?\n",
        "   \"ERR\":\"https://drive.google.com/file/d/1rEo1thMNJTgFeTzTOfI11_FPSqMgbHSL/view?usp=sharing\",\n",
        "   \"ERS\":\"https://drive.google.com/file/d/1ts-t7bOH9DfKj0q0v35nMgKHgVT0ZjyG/view?usp=sharing\",\n",
        "   \"FRA\":\"https://drive.google.com/file/d/1yLacRGW7JtlFWV5asEXHpAToClL38D64/view?usp=sharing\",\n",
        "   \"HEG\":\"https://drive.google.com/file/d/1zoJKEIrsCD1XxglgPJkEygumev1xRQ3U/view?usp=sharing\",\n",
        "   \"MEL\":\"https://drive.google.com/file/d/1Wdy3DDnWfUysXjcIFFq12UFW7tlTYDT2/view?usp=sharing\",\n",
        "   \"ORA\":\"https://drive.google.com/file/d/19_oYwCAaPfP6vMuqUnAzIQAa39Brxhfi/view?usp=sharing\",\n",
        "   \"OSV\":\"https://drive.google.com/file/d/1VJCCZN1iwBK2Nzh_PHC9xvw63xiLuXXI/view?usp=sharing\",\n",
        "#   \"PEG\":\"https://drive.google.com/file/d/1YUWH4IaTnmcoIAavZq8HyXByJxO7_zBg/view?usp=sharing\", # Assez de données?\n",
        "   \"PET\":\"https://drive.google.com/file/d/13bMkvr_1mRz1TuOcX8-c-LfTSIsNKrve/view?usp=sharing\",\n",
        "   \"PIB\":\"https://drive.google.com/file/d/17J9g1xm6-ji52k2pgJr7mUrJdS1ASSqP/view?usp=sharing\",\n",
        "#   \"PID\":\"https://drive.google.com/file/d/12xswrf4pDmTAcYZDAY9D-0HniLjGJCxp/view?usp=sharing\", # Assez de données?\n",
        "   \"PIR\":\"https://drive.google.com/file/d/1qny4meuoT-HYZ_KTyPQbQnzLhebkgkfU/view?usp=sharing\",\n",
        "   \"PRU\":\"https://drive.google.com/file/d/1xQWHQvIbwRRBoi2F27q22_drUeM8m3S8/view?usp=sharing\",\n",
        "   \"SAB\":\"https://drive.google.com/file/d/1ol2mlYAz5bMfQkwqcnxhCOg4avftYtRe/view?usp=sharing\",\n",
        "   \"THO\":\"https://drive.google.com/file/d/1_mI0saGpfxb4wnhElCzxg0WU4OiFHkfP/view?usp=sharing\",\n",
        "  \n",
        "}\n",
        "data_zip_urls_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJSQQXuACJNw"
      },
      "outputs": [],
      "source": [
        "# Création des répertoires de données\n",
        "# Nous allons créer un répertoire de base `src` et des répertoiresnpour les données \n",
        "# d'entrainement, de validation et de test pour chaque étiquette cible\n",
        "\n",
        "try:\n",
        "    os.mkdir(\"/content/src/\")\n",
        "except OSError:\n",
        "    pass\n",
        "try:\n",
        "    os.mkdir(\"/content/lab_id_arbres/\")\n",
        "except OSError:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pC8uagBC6fi"
      },
      "outputs": [],
      "source": [
        "# Référence: https://colab.research.google.com/notebooks/io.ipynb\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH9UmLEjCJNz"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owjeRkDGO203"
      },
      "outputs": [],
      "source": [
        "nbr_classes = 0\n",
        "for arbre_id in data_zip_urls_dict.keys():\n",
        "    url = data_zip_urls_dict[arbre_id]\n",
        "    id_fichier = url.split('/')[5]\n",
        "    fichier = drive.CreateFile({'id':id_fichier})\n",
        "    nom_fichier = arbre_id + \".zip\"\n",
        "    # télécharger le fichier nom_fichier\n",
        "    fichier.GetContentFile(\"/content/src/\" + nom_fichier)\n",
        "    print(\"Fichier \" + nom_fichier + \" téléchargé\")\n",
        "    zip_ref = zipfile.ZipFile(\"/content/src/\" + nom_fichier, 'r')\n",
        "    zip_ref.extractall(\"/content/src\")\n",
        "    zip_ref.close()\n",
        "    print(\"Fichier \" + nom_fichier + \" décompressé\")\n",
        "    try:\n",
        "        os.remove(\"/content/src/\"+nom_fichier)\n",
        "        print(\"Fichier \" + nom_fichier + \" effacé\")\n",
        "    except:\n",
        "        print(\"?\")\n",
        "    nbr_classes += 1\n",
        "shutil.rmtree('/content/src/__MACOSX')\n",
        "print(\"nbr_classes:\",nbr_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-2k_K6xc9Wq"
      },
      "source": [
        "## Répartition des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOcFvy9DP25S"
      },
      "outputs": [],
      "source": [
        "!pip3 install split-folders tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lETVeD8CJO5"
      },
      "outputs": [],
      "source": [
        "# Répartition des données d'entraînement, de validation et de tests\n",
        "import splitfolders\n",
        "import pathlib\n",
        "\n",
        "#### input dataset that want to split\n",
        "input_folder = \"/content/src\"\n",
        "output_folder= \"/content/lab_id_arbres\"\n",
        "# => train, val, test\n",
        "\n",
        "image_count = len(list(pathlib.Path(input_folder).glob('*/*.jpg')))\n",
        "print(\"Nombre total d'images:\",image_count)\n",
        "\n",
        "splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio = (0.80, 0.15, 0.05))\n",
        "\n",
        "print(\"\\nRépartition des données terminée!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhR7o-0vdRrp"
      },
      "source": [
        "### Visualisation d'un échantillon des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkdEBY8nCJO7",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pic_index = 4\n",
        "\n",
        "TRAINING_DIR = \"/content/lab_id_arbres/train/\"\n",
        "\n",
        "for arbre_id in data_ecorces.keys():\n",
        "    try:\n",
        "        dir_path = os.path.join(TRAINING_DIR,arbre_id+os.sep)\n",
        "        liste_fichiers = os.listdir(os.path.join(TRAINING_DIR,arbre_id+os.sep))\n",
        "        next_two_pics = [os.path.join(dir_path, fname) for fname in liste_fichiers[0:pic_index]]\n",
        "        fig = plt.figure(figsize=(12,4))\n",
        "        print(\"_\"*90)\n",
        "        print(arbre_id,noms_arbres[data_ecorces[arbre_id]].split('-')[1])\n",
        "        for i, img_path in enumerate(next_two_pics):\n",
        "            print(img_path)\n",
        "            img = mpimg.imread(img_path)\n",
        "            plt.subplot(1,pic_index,i+1)\n",
        "            plt.imshow(img)\n",
        "            plt.axis('Off')\n",
        "        plt.show()\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dah-w5madZk2"
      },
      "source": [
        "### Création de flux de lots de données pour l'entraînement\n",
        "\n",
        "Chargeons ces images en mémoire en créant des `tf.data.dataset` à l'aide de l'utilitaire `tf.keras.utils.image_dataset_from_directory`. \n",
        "\n",
        "tf.data.Dataset prend en charge l'écriture de chaîne de traitement d'entrée de données efficaces. L'itération se produit dans un flux continu, de sorte que l'ensemble de données complet n'a pas besoin de tenir dans la mémoire."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "print(\"Keras version:\",keras.__version__)\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\",tf.__version__)"
      ],
      "metadata": {
        "id": "_Yd0XbAuiaq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXj44_jSCJQN"
      },
      "outputs": [],
      "source": [
        "TRAINING_DIR = \"/content/lab_id_arbres/train/\"\n",
        "VALIDATION_DIR = \"/content/lab_id_arbres/val\"\n",
        "TESTING_DIR = \"/content/lab_id_arbres/test/\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (150, 150)\n",
        "\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(TRAINING_DIR,\n",
        "                                                            batch_size=BATCH_SIZE,\n",
        "                                                            image_size=IMG_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"type(train_dataset):\",type(train_dataset))\n",
        "examples, labels = next(iter(train_dataset))\n",
        "print(\"examples.shape:\",examples.shape)\n",
        "print(\"len(examples):\",len(examples))\n",
        "print(\"labels.shape:\",labels.shape)"
      ],
      "metadata": {
        "id": "bo13n_NPizIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_classes = train_dataset.class_names\n",
        "print(id_classes)\n",
        "print(len(id_classes))"
      ],
      "metadata": {
        "id": "3oUWdWjvmdkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = tf.keras.utils.image_dataset_from_directory(VALIDATION_DIR,\n",
        "                                                                 batch_size=BATCH_SIZE,\n",
        "                                                                 image_size=IMG_SIZE)"
      ],
      "metadata": {
        "id": "iDbUSJq0lwXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = tf.keras.utils.image_dataset_from_directory(TESTING_DIR,\n",
        "                                                           batch_size=BATCH_SIZE,\n",
        "                                                           image_size=IMG_SIZE)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "a1Sp388MlxGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FJpsONdWvda"
      },
      "outputs": [],
      "source": [
        "print('Number of training batches: %d' % tf.data.experimental.cardinality(train_dataset))\n",
        "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
        "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prétraitement des données"
      ],
      "metadata": {
        "id": "HAvjjyGdfvwN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5Vpk0ZgeSK0"
      },
      "source": [
        "### Normalisation des données"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resize_and_rescale = keras.Sequential([\n",
        "  keras.layers.Resizing(150,150),\n",
        "  keras.layers.Rescaling(1./255)\n",
        "])\n"
      ],
      "metadata": {
        "id": "kZCd17h9vMhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amplification des données"
      ],
      "metadata": {
        "id": "QZsG8B4MvQRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  keras.layers.RandomFlip(\"horizontal\"),\n",
        "  keras.layers.RandomFlip(\"vertical\"),\n",
        "  keras.layers.RandomRotation(0.1),\n",
        "  keras.layers.RandomZoom(0.3),\n",
        "  keras.layers.RandomContrast(0.3),\n",
        "])"
      ],
      "metadata": {
        "id": "9h6v9x2GE0O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_dataset.take(1):\n",
        "  fig.clear()\n",
        "  for i in range(9):\n",
        "    augmented_images = data_augmentation(images)\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "    # plt.imshow(augmented_images[0], cmap=\"gray\")"
      ],
      "metadata": {
        "id": "ViNSQKJCQuyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def pretraitement(ds, shuffle=False, augment=False):\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(1000)\n",
        "    \n",
        "  # Batch all datasets.\n",
        "  # ds = ds.batch(batch_size)\n",
        "\n",
        "  # Use data augmentation only on the training set.\n",
        "  if augment:\n",
        "    ds = ds.map(lambda x, y: (data_augmentation(x,training=True), y),\n",
        "                num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "\n",
        "  # Resize and rescale all datasets.\n",
        "  ds = ds.map(lambda x, y: (resize_and_rescale(x), y),\n",
        "              num_parallel_calls=AUTOTUNE\n",
        "  )\n",
        "\n",
        "  # Use buffered prefetching on all datasets.\n",
        "  return ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"Fonction pretraitement prête!\")"
      ],
      "metadata": {
        "id": "bpINnJ-Tf3G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMjtfK7HeVF-"
      },
      "outputs": [],
      "source": [
        "print(\"type(train_dataset):\",type(train_dataset))\n",
        "normalized_train_dataset = pretraitement(train_dataset,\n",
        "                                         shuffle=True,\n",
        "                                         augment=True)\n",
        "print(\"type(normalized_train_dataset):\",type(normalized_train_dataset))\n",
        "image_batch, labels_batch = next(iter(normalized_train_dataset))\n",
        "first_image = image_batch[0]\n",
        "# Notice the pixel values are now in `[0,1]`.\n",
        "print(np.min(first_image), np.max(first_image))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples, labels = next(iter(normalized_train_dataset))\n",
        "print(\"examples.shape:\",examples.shape)\n",
        "print(\"len(examples):\",len(examples))\n",
        "print(\"labels.shape:\",labels.shape)"
      ],
      "metadata": {
        "id": "RNPAUcTVjPEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYzwVlb1Lybv"
      },
      "outputs": [],
      "source": [
        "# normalized_validation_dataset = validation_dataset.map(lambda x, y: (resize_and_rescale(x), y))\n",
        "print(\"type(validation_dataset):\",type(validation_dataset))\n",
        "normalized_validation_dataset = pretraitement(validation_dataset,\n",
        "                                              shuffle=False,\n",
        "                                              augment=False)\n",
        "print(\"type(normalized_validation_dataset):\",type(normalized_validation_dataset))\n",
        "image_batch, labels_batch = next(iter(normalized_validation_dataset))\n",
        "first_image = image_batch[0]\n",
        "# Notice the pixel values are now in `[0,1]`.\n",
        "print(np.min(first_image), np.max(first_image))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples, labels = next(iter(normalized_validation_dataset))\n",
        "print(\"examples.shape:\",examples.shape)\n",
        "print(\"len(examples):\",len(examples))\n",
        "print(\"labels.shape:\",labels.shape)"
      ],
      "metadata": {
        "id": "BvAuJ89LqhXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIX-cGuYlhpf"
      },
      "outputs": [],
      "source": [
        "print(\"type(test_dataset):\",type(test_dataset))\n",
        "normalized_test_dataset = test_dataset.map(lambda x, y: (resize_and_rescale(x), y))\n",
        "# normalized_test_dataset = pretraitement(test_dataset,\n",
        "#                                         shuffle=False,\n",
        "#                                         augment=False)\n",
        "print(\"type(normalized_test_dataset):\",type(normalized_test_dataset))\n",
        "image_batch, labels_batch = next(iter(normalized_test_dataset))\n",
        "first_image = image_batch[0]\n",
        "# Notice the pixel values are now in `[0,1]`.\n",
        "print(np.min(first_image), np.max(first_image))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples, labels = next(iter(normalized_test_dataset))\n",
        "print(\"examples.shape:\",examples.shape)\n",
        "print(\"len(examples):\",len(examples))\n",
        "print(\"labels.shape:\",labels.shape)"
      ],
      "metadata": {
        "id": "TuEwDfUJqtYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqXiGhl_SVUb"
      },
      "source": [
        "# Création d'un modèle d'apprentissage par transfert\n",
        "\n",
        "## Importation d'un modèle inception pré-entraîné"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD6505ZACJNc"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSgVMlyGCJP9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# Load weights pre-trained on ImageNet\n",
        "pre_trained_model = InceptionV3(weights='imagenet',  \n",
        "                                input_shape = (150, 150, 3), \n",
        "                                include_top = False)\n",
        "\n",
        "# Create new data input\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "\n",
        "# pre_trained_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyumQbPErKrs"
      },
      "outputs": [],
      "source": [
        "pre_trained_model.trainable = True\n",
        "\n",
        "# Let's take a look to see how many layers are in the pretrained model\n",
        "nbr_layers_pretrained_model = len(pre_trained_model.layers)\n",
        "print(\"Nombre de couches dans le modèle pré-entraîné d'origine: \", nbr_layers_pretrained_model)\n",
        "\n",
        "# How to get the layer index from the layer name\n",
        "# https://www.thetopsites.net/article/50151157.shtml\n",
        "layer_names = [layer.name for layer in pre_trained_model.layers]\n",
        "last_layer_name = layer_names[-1]\n",
        "print(\"Nom de la dernière couche du modèle pré-entraîné complet:\",last_layer_name)\n",
        "last_layer_index = layer_names.index(last_layer_name)\n",
        "# Choix d'une nouvelle couche de sortie par essai / erreur\n",
        "last_layer_name = 'mixed5'\n",
        "# last_layer_name = 'mixed10'\n",
        "print(\"Choix empirique de la dernière du modèle pré-entraîné:\",last_layer_name)\n",
        "last_layer_index = layer_names.index(last_layer_name)\n",
        "print(\"Index de la dernière couche du modèle pré-entraîné:\",last_layer_index)\n",
        "# Choix de la dernière couche non-entraînable ou dernière couche « gelée »\n",
        "last_layer_frozen_name = 'mixed3'\n",
        "# last_layer_frozen_name = 'mixed10'\n",
        "print(\"Choix empirique de la dernière couche non-entraînable:\",last_layer_frozen_name)\n",
        "last_layer_frozen_index = layer_names.index(last_layer_frozen_name)\n",
        "print(\"Index de la dernière couche non-entraînable:\",last_layer_frozen_index)\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = last_layer_frozen_index\n",
        "print(\"Nombre de couches non entraînables dans le modèle préentraîné: \", fine_tune_at )\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in pre_trained_model.layers[:fine_tune_at]:\n",
        "    layer.trainable =  False\n",
        "\n",
        "print(\"Nombre de couches entraînables dans le modèle préentraîné: \", last_layer_index-fine_tune_at)\n",
        "last_layer = pre_trained_model.get_layer(last_layer_name)\n",
        "print('Dimensions de la dernière couche: ', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eeNLUIfCJQB"
      },
      "outputs": [],
      "source": [
        "# Add a dropout rate of 0.1\n",
        "outputs = layers.Dropout(0.1)(last_output)\n",
        "# Flatten the output layer to 1 dimension\n",
        "outputs = layers.Flatten()(outputs)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "outputs = layers.Dense(1024, activation='relu')(outputs)\n",
        "# Add a dropout rate of 0.2\n",
        "outputs = layers.Dropout(0.2)(outputs)      \n",
        "# Add a final softmax layer for classification\n",
        "# *** IMPORTANT *** 20 classes\n",
        "number_of_target_class = 20\n",
        "outputs = layers.Dense(number_of_target_class, activation='softmax')(outputs)           \n",
        "\n",
        "transfer_model = Model(pre_trained_model.input, outputs) \n",
        "print(\"Number of layers in the learning transfer model: \", len(transfer_model.layers))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96gJIRE0CJQE",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# transfer_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZxGej8rCJQG"
      },
      "outputs": [],
      "source": [
        "len(transfer_model.trainable_variables)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29UmKt85eFmH"
      },
      "source": [
        "## Compilation du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHcwa9acCJQM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# According to tf.keras.utils.image_dataset_from_directory documentation, 'int' is the default label_mode\n",
        "# 'int': means that the labels are encoded as integers (e.g. for sparse_categorical_crossentropy loss)\n",
        "# So, by default, tf.keras.utils.image_dataset_from_directory will create a set of labels for the dataset \n",
        "# as integer that go from 1 to the number of classes in the dataset.\n",
        "# In this case, the model should be compiled with a 'sparse_categorical_crossentropy' loss.\n",
        "transfer_model.compile(optimizer = RMSprop(learning_rate=0.0001), \n",
        "              loss = 'sparse_categorical_crossentropy', \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "print(\"Modèle compilé!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFh_MKBieH_W"
      },
      "source": [
        "## Entraînement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaQ862pnCJQQ"
      },
      "outputs": [],
      "source": [
        "initial_epochs = 10\n",
        "fine_tune_epochs = 30\n",
        "total_epochs =  initial_epochs + fine_tune_epochs\n",
        "\n",
        "# Callbacks are passed to the model via the callbacks argument in fit, \n",
        "# which takes a list of callbacks. You can pass any number of callbacks.\n",
        "callbacks_list = [\n",
        "                  keras.callbacks.ModelCheckpoint(\n",
        "                      filepath=\"id_arbres_CNN_transfert_amplification.keras\",\n",
        "                      save_best_only=True,\n",
        "                      monitor=\"val_loss\"),\n",
        "                  # Interrupts training when improvement stops\n",
        "                  keras.callbacks.EarlyStopping(\n",
        "                      # Monitors the model’s validation accuracy\n",
        "                      monitor='val_accuracy',\n",
        "                      # Interrupts training when accuracy has stopped \n",
        "                      # improving for more than 3 epoch (that is, 4 epochs)\n",
        "                      patience=10,\n",
        "                      verbose=2),\n",
        "                  keras.callbacks.ReduceLROnPlateau(\n",
        "                      # Monitors the model’s validation loss\n",
        "                      monitor='val_loss',\n",
        "                      # Divides the learning rate by 2 when triggered\n",
        "                      factor=0.5,\n",
        "                      # The callback is triggered after the validation loss\n",
        "                      # has stopped improving for 1 epoch.\n",
        "                      patience=3,\n",
        "                      verbose=2) \n",
        "]\n",
        "history_fine = transfer_model.fit(normalized_train_dataset,\n",
        "                                  validation_data = normalized_validation_dataset,\n",
        "                                  epochs = total_epochs,\n",
        "                                  callbacks=callbacks_list,\n",
        "                                  verbose = 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7kJQvyVCJQR"
      },
      "outputs": [],
      "source": [
        "# dernière couche 'mixed5', dernière couche gelée 'mixed3'\n",
        "# 40 époques max + amplification\n",
        "# Test accuracy : 80 à 84%\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history_fine.history['accuracy']\n",
        "val_acc = history_fine.history['val_accuracy']\n",
        "loss = history_fine.history['loss']\n",
        "val_loss = history_fine.history['val_loss']\n",
        "epochs = range(len(acc))\n",
        "golden_number = 1.618\n",
        "height = 6\n",
        "length = int(golden_number * height)\n",
        "plt.figure(figsize=(length,height))\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy - augmentation on training data with dropout')\n",
        "plt.legend(loc=0)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM-0HM1iq7BF"
      },
      "outputs": [],
      "source": [
        "# dernière couche 'mixed10', dernière couche gelée 'mixed10'\n",
        "# 20 epochs\n",
        "# Test accuracy : 0.5871559381484985"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HG1NiuAp-SR"
      },
      "outputs": [],
      "source": [
        "erreur, exactitude = transfer_model.evaluate(normalized_test_dataset)\n",
        "print(\"Exactitude données de test accuracy:   %0.2f\" % exactitude)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"type(test_dataset):\",type(test_dataset))\n",
        "print(\"type(normalized_test_dataset):\",type(normalized_test_dataset))\n",
        "image_batch, labels_batch = next(iter(normalized_test_dataset))\n",
        "first_image = image_batch[0]\n",
        "# Notice the pixel values are now in `[0,1]`.\n",
        "print(np.min(first_image), np.max(first_image))"
      ],
      "metadata": {
        "id": "9ziDKb3eQtdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples, labels = next(iter(normalized_test_dataset))\n",
        "print(\"examples.shape:\",examples.shape)\n",
        "print(\"len(examples):\",len(examples))\n",
        "print(\"labels.shape:\",labels.shape)"
      ],
      "metadata": {
        "id": "6ALuJEj0h2MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = transfer_model.predict(normalized_test_dataset)\n",
        "predictions_index = np.argmax(predictions, axis=1)\n",
        "predictions_index = [(etiquette+1) for etiquette in predictions_index]\n",
        "print(predictions_index)"
      ],
      "metadata": {
        "id": "cW4eB97P1YkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vraies_etiquettes_index = list(np.concatenate([etiquette for image, etiquette in normalized_test_dataset], axis=0))\n",
        "vraies_etiquettes_index = [(etiquette+1) for etiquette in vraies_etiquettes_index]\n",
        "print(vraies_etiquettes_index)"
      ],
      "metadata": {
        "id": "vD_Qpaco3BSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "score = metrics.accuracy_score(vraies_etiquettes_index, predictions_index)\n",
        "print(\"Exactitude:   %0.2f\" % score)"
      ],
      "metadata": {
        "id": "jH1IJNCvIzqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/65618137/confusion-matrix-for-multiple-classes-in-python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "### Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true=np.argmax(vraies_etiquettes_index, axis=-1)\n",
        "y_true= vraies_etiquettes_index\n",
        "y_pred = predictions_index\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "## Get Class Labels\n",
        "class_names = list(id_classes)\n",
        "\n",
        "# Plot confusion matrix in a beautiful manner\n",
        "fig = plt.figure(figsize=(16, 14))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Classes prédites', fontsize=20)\n",
        "ax.xaxis.set_label_position('bottom')\n",
        "plt.xticks(rotation=90)\n",
        "ax.xaxis.set_ticklabels(class_names, fontsize = 10)\n",
        "ax.xaxis.tick_bottom()\n",
        "\n",
        "ax.set_ylabel('Vraies classes', fontsize=20)\n",
        "ax.yaxis.set_ticklabels(class_names, fontsize = 10)\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "plt.title('Matrice de confusion', fontsize=20)\n"
      ],
      "metadata": {
        "id": "NFjpa-BfDdQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Matrice de confusion',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    plt.figure(figsize=(14,12))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Matrice de confusion normalisée\")\n",
        "    else:\n",
        "        print('Matrice de confusion non normalisée')\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Vraies étiquettes')\n",
        "    plt.xlabel('Étiquettes prédites')\n",
        "\n",
        "cm = metrics.confusion_matrix(vraies_etiquettes_index, predictions_index)\n",
        "plot_confusion_matrix(cm, classes=class_names)"
      ],
      "metadata": {
        "id": "zrL3kyhDHYbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Résultats\n",
        "\n",
        "Les résultats dans une fourchette de 82 à 85% ne sont pas excellents. Avec 32 Go de données l'équipe de chercheurs de l'Université Laval à Québec avait obtenu 93%. Mais avec 1.5 Go de données (21 fois moins de données), on ne peut pas faire de miracle... \n"
      ],
      "metadata": {
        "id": "ipodLWa-D58J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sauvegarde du modèle entraîné"
      ],
      "metadata": {
        "id": "m_1fZFg3Pa3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir Saved_Models\n",
        "base_path = \"Saved_Models/\""
      ],
      "metadata": {
        "id": "4R8Uxcb0PnJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# Add time_stamp for versioning\n",
        "time_stamp = str(int(time.time()))\n",
        "saved_model_path = os.path.join(base_path, time_stamp)\n",
        "tf.saved_model.save(transfer_model, saved_model_path)\n"
      ],
      "metadata": {
        "id": "NAjmgCKQPiq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_path"
      ],
      "metadata": {
        "id": "WlY6zgh1emrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Télécharger localement le modèle entraîné"
      ],
      "metadata": {
        "id": "TaAq8J8GQqlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vhuDoqFfjt7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"saved_model_\"+time_stamp,'zip', saved_model_path)"
      ],
      "metadata": {
        "id": "jgTcsC7SeQ8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"saved_model_\"+time_stamp+\".zip\")"
      ],
      "metadata": {
        "id": "X3pDuFekeE9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvgAD9crCJQU"
      },
      "outputs": [],
      "source": [
        "print(\"Fin de l'exécution du carnet IPython\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Id_arbre_reseau_convolutif-transfert_paufinage-colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}