{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVQ7Ein3FCZG"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaudeCoulombe/VIARENA/blob/master/Labos/Lab-Identification_Arbres/Identification_arbre_reseau_convolutif_et_transfert_colab.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "### Rappel - Fonctionnement d'un carnet web iPython\n",
        "\n",
        "* Pour exécuter le code contenu dans une cellule d'un carnet iPython, cliquez dans la cellule et faites (⇧↵, shift-enter) \n",
        "* Le code d'un carnet iPython s'exécute séquentiellement de haut en bas de la page. Souvent, l'importation d'une bibliothèque Python ou l'initialisation d'une variable est préalable à l'exécution d'une cellule située plus bas. Il est donc recommandé d'exécuter les cellules en séquence. Enfin, méfiez-vous des retours en arrière qui peuvent réinitialiser certaines variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "# Identification d'arbres à partir de leur écorce\n",
        "## Réseau convolutif et apprentissage par transfert\n",
        "\n",
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BZSlp3DAjdYf"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq38SFqsf7b0"
      },
      "source": [
        "## Apprentissage par transfert et paufinage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snh2vdTCTBhy"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax2GDFiCCJNt"
      },
      "outputs": [],
      "source": [
        "# Note that we did not use Acer platanoides (2), Pinus rigida (15) and Populus grandidentata (18)\n",
        "# since we did not collect enough images in these categories to obtain meaningful results.\n",
        "data_ecorces = {\n",
        "    'SAB': 1,  # Pas assez d'espace disque\n",
        "    'ERB': 2,  # Pas assez de spécimens - seulement 1\n",
        "    'ERR': 3, \n",
        "    'ERS': 4, \n",
        "    'BOJ': 5, \n",
        "    'BOP': 6,\n",
        "    'HEG': 7,  # Pas assez d'espace disqe\n",
        "    'FRA': 8, \n",
        "    'MEL': 9,  # Pas assez d'espace disque\n",
        "    'OSV': 10, # Pas assez d'espace disque\n",
        "    'EPO': 11,\n",
        "    'EPB': 12,\n",
        "    'EPN': 13,\n",
        "    'EPR': 14,\n",
        "    'PID': 15, # Pas assez de spécimens - seulement 4\n",
        "    'PIR': 16, # Pas assez d'espace disque\n",
        "    'PIB': 17, # Pas assez d'espace disque\n",
        "    'PEG': 18, # Pas assez de spécimens - seulement 3\n",
        "    'PET': 19, # Pas assez d'espace disque\n",
        "    'CHR': 20,\n",
        "    'THO': 21, # Pas assez d'espace disque\n",
        "    'PRU': 22, # Pas assez d'espace disque\n",
        "    'ORA': 23  # Pas assez d'espace disque\n",
        "}\n",
        "\n",
        "noms_arbres = {\n",
        "            1: '\\emph{Abies balsamea} - Sapin Baumier - Balsam fir',\n",
        "            2: '\\emph{Acer platanoides} - Érable de Norvège - Norway maple',\n",
        "            3: '\\emph{Acer rubrum} - Érable rouge - Red maple',\n",
        "            4: '\\emph{Acer saccharum} - Érable à sucre - Sugar maple',\n",
        "            5: '\\emph{Betula alleghaniensis} - Bouleau jaune - Yellow birch',\n",
        "            6: '\\emph{Betula papyrifera} - Bouleau à papier - White birch',\n",
        "            7: '\\emph{Fagus grandifolia} - Hêtre à grandes feuilles - American beech',\n",
        "            8: \"\\emph{Fraxinus americana} - Frêne d'Amérique - White ash\",\n",
        "            9: '\\emph{Larix laricina} - Mélèze - Tamarack',\n",
        "            10: '\\emph{Ostrya virginiana} - Ostryer de Virginie - American hophornbeam',\n",
        "            11: '\\emph{Picea abies} - Épinette de Norvège - Norway spruce',\n",
        "            12: '\\emph{Picea glauca} - Épinette blanche - White spruce',\n",
        "            13: '\\emph{Picea mariana} - Épinette noire - Black spruce',\n",
        "            14: '\\emph{Picea rubens} - Épinette rouge - Red spruce',\n",
        "            15: '\\emph{Pinus rigida} - Pin rigide - Pitch pine',\n",
        "            16: '\\emph{Pinus resinosa} - Pin rouge - Red pine',\n",
        "            17: '\\emph{Pinus strobus} - Pin blanc - Eastern white pine',\n",
        "            18: '\\emph{Populus grandidentata} - Peuplier à grandes dents - Big-tooth aspen',\n",
        "            19: '\\emph{Populus tremuloides} - Peuplier faux tremble - Quaking aspen',\n",
        "            20: '\\emph{Quercus rubra} - Chêne rouge - Northern red oak',\n",
        "            21: '\\emph{Thuja occidentalis} - Thuya occidental - Northern white cedar',\n",
        "            22: '\\emph{Tsuga canadensis} - Pruche du Canada - Eastern Hemlock',\n",
        "            23: \"\\emph{Ulmus americana} - Orme d'Amérique - American elm\"\n",
        "        }\n",
        "print(\"Code executed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8ADcq2QPbu6"
      },
      "outputs": [],
      "source": [
        "data_zip_urls_dict = {\n",
        "   \"BOJ\":\"https://drive.google.com/file/d/1d2zxg2pt5S8UJIK-E7IuWfGN0d1kxxMw/view?usp=sharing\",\n",
        "   \"BOP\":\"https://drive.google.com/file/d/12cg6UO4HLnjk5fE_KXtrgdC2s8uGh4Zp/view?usp=sharing\",\n",
        "   \"CHR\":\"https://drive.google.com/file/d/1Nq19-I-Q577KXMTFrkhlJDhMfclh0cWn/view?usp=sharing\",\n",
        "   \"EPB\":\"https://drive.google.com/file/d/1K_Ncw8VEiuDZ_iJDbYToMq-GO5dzKHns/view?usp=sharing\",\n",
        "   \"EPN\":\"https://drive.google.com/file/d/1S309DYmg76SrIA89aVQWXCMwm6CzhN8b/view?usp=sharing\",\n",
        "   \"EPO\":\"https://drive.google.com/file/d/1fTKEcpYgmRg4spUpcH0FAiAnoRgANafL/view?usp=sharing\",\n",
        "   \"EPR\":\"https://drive.google.com/file/d/1qRhtZ8LZjH_45fxetG7swg3ok3znk8CJ/view?usp=sharing\",\n",
        "   \"ERB\":\"https://drive.google.com/file/d/1ighbGniKAT_GrPm4RtsIAuN1STg9sjR9/view?usp=sharing\", # Assez de données?\n",
        "   \"ERR\":\"https://drive.google.com/file/d/1rEo1thMNJTgFeTzTOfI11_FPSqMgbHSL/view?usp=sharing\",\n",
        "   \"ERS\":\"https://drive.google.com/file/d/1ts-t7bOH9DfKj0q0v35nMgKHgVT0ZjyG/view?usp=sharing\",\n",
        "   \"FRA\":\"https://drive.google.com/file/d/1yLacRGW7JtlFWV5asEXHpAToClL38D64/view?usp=sharing\",\n",
        "   \"HEG\":\"https://drive.google.com/file/d/1zoJKEIrsCD1XxglgPJkEygumev1xRQ3U/view?usp=sharing\",\n",
        "   \"MEL\":\"https://drive.google.com/file/d/1Wdy3DDnWfUysXjcIFFq12UFW7tlTYDT2/view?usp=sharing\",\n",
        "   \"ORA\":\"https://drive.google.com/file/d/19_oYwCAaPfP6vMuqUnAzIQAa39Brxhfi/view?usp=sharing\",\n",
        "   \"OSV\":\"https://drive.google.com/file/d/1VJCCZN1iwBK2Nzh_PHC9xvw63xiLuXXI/view?usp=sharing\",\n",
        "   \"PEG\":\"https://drive.google.com/file/d/1YUWH4IaTnmcoIAavZq8HyXByJxO7_zBg/view?usp=sharing\", # Assez de données?\n",
        "   \"PET\":\"https://drive.google.com/file/d/13bMkvr_1mRz1TuOcX8-c-LfTSIsNKrve/view?usp=sharing\",\n",
        "   \"PIB\":\"https://drive.google.com/file/d/13bMkvr_1mRz1TuOcX8-c-LfTSIsNKrve/view?usp=sharing\",\n",
        "   \"PID\":\"https://drive.google.com/file/d/12xswrf4pDmTAcYZDAY9D-0HniLjGJCxp/view?usp=sharing\", # Assez de données?\n",
        "   \"PIR\":\"https://drive.google.com/file/d/1qny4meuoT-HYZ_KTyPQbQnzLhebkgkfU/view?usp=sharing\",\n",
        "   \"PRU\":\"https://drive.google.com/file/d/1xQWHQvIbwRRBoi2F27q22_drUeM8m3S8/view?usp=sharing\",\n",
        "   \"SAB\":\"https://drive.google.com/file/d/1ol2mlYAz5bMfQkwqcnxhCOg4avftYtRe/view?usp=sharing\",\n",
        "   \"THO\":\"https://drive.google.com/file/d/1_mI0saGpfxb4wnhElCzxg0WU4OiFHkfP/view?usp=sharing\",\n",
        "  \n",
        "}\n",
        "data_zip_urls_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJSQQXuACJNw"
      },
      "outputs": [],
      "source": [
        "# Création des répertoires de données\n",
        "# Nous allons créer un répertoire de base `src` et des répertoiresnpour les données \n",
        "# d'entrainement, de validation et de test pour chaque étiquette cible\n",
        "\n",
        "try:\n",
        "    os.mkdir(\"/tmp/src/\")\n",
        "except OSError:\n",
        "    pass\n",
        "try:\n",
        "    os.mkdir(\"/tmp/lab_id_arbres/\")\n",
        "except OSError:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pC8uagBC6fi"
      },
      "outputs": [],
      "source": [
        "# Référence: https://colab.research.google.com/notebooks/io.ipynb\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH9UmLEjCJNz"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owjeRkDGO203"
      },
      "outputs": [],
      "source": [
        "nbr_classes = 0\n",
        "for arbre_id in data_zip_urls_dict.keys():\n",
        "    url = data_zip_urls_dict[arbre_id]\n",
        "    id_fichier = url.split('/')[5]\n",
        "    fichier = drive.CreateFile({'id':id_fichier})\n",
        "    nom_fichier = arbre_id + \".zip\"\n",
        "    # télécharger le fichier nom_fichier\n",
        "    fichier.GetContentFile(\"/tmp/src/\" + nom_fichier)\n",
        "    print(\"Fichier \" + nom_fichier + \" téléchargé\")\n",
        "    zip_ref = zipfile.ZipFile(\"/tmp/src/\" + nom_fichier, 'r')\n",
        "    zip_ref.extractall(\"/tmp/src\")\n",
        "    zip_ref.close()\n",
        "    print(\"Fichier \" + nom_fichier + \" décompressé\")\n",
        "    try:\n",
        "        os.remove(\"/tmp/src/\"+nom_fichier)\n",
        "        print(\"Fichier \" + nom_fichier + \" effacé\")\n",
        "    except:\n",
        "        print(\"?\")\n",
        "    nbr_classes += 1\n",
        "shutil.rmtree('/tmp/src/__MACOSX')\n",
        "print(\"nbr_classes:\",nbr_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOcFvy9DP25S"
      },
      "outputs": [],
      "source": [
        "!pip3 install split-folders tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lETVeD8CJO5"
      },
      "outputs": [],
      "source": [
        "# Répartition des données d'entraînement et de tests\n",
        "import splitfolders\n",
        "\n",
        "#### input dataset that want to split\n",
        "input_folder = \"/tmp/src\"\n",
        "output_folder= \"/tmp/lab_id_arbres\"\n",
        "# => train, val, test\n",
        "\n",
        "splitfolders.ratio(input_folder, output= output_folder, seed=42, ratio = (0.95, 0.0, 0.05))\n",
        "\n",
        "print(\"Répartition des données terminée!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkdEBY8nCJO7",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pic_index = 4\n",
        "\n",
        "TRAINING_DIR = \"/tmp/lab_id_arbres/train/\"\n",
        "\n",
        "for arbre_id in data_ecorces.keys():\n",
        "    try:\n",
        "        dir_path = os.path.join(TRAINING_DIR,arbre_id+os.sep)\n",
        "        liste_fichiers = os.listdir(os.path.join(TRAINING_DIR,arbre_id+os.sep))\n",
        "        next_two_pics = [os.path.join(dir_path, fname) for fname in liste_fichiers[0:pic_index]]\n",
        "        fig = plt.figure(figsize=(12,4))\n",
        "        print(\"_\"*90)\n",
        "        print(arbre_id,noms_arbres[data_ecorces[arbre_id]].split('-')[1])\n",
        "        for i, img_path in enumerate(next_two_pics):\n",
        "            print(img_path)\n",
        "            img = mpimg.imread(img_path)\n",
        "            plt.subplot(1,pic_index,i+1)\n",
        "            plt.imshow(img)\n",
        "            plt.axis('Off')\n",
        "        plt.show()\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqXiGhl_SVUb"
      },
      "source": [
        "# Création d'un modèle d'apprentissage par transfert\n",
        "\n",
        "## Importation d'un modèle inception pré-entraîné"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD6505ZACJNc"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSgVMlyGCJP9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import keras\n",
        "print(\"Keras version:\",keras.__version__)\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\",tf.__version__)\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "pre_trained_model = InceptionV3(weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "                                input_shape = (150, 150, 3), \n",
        "                                include_top = False)\n",
        "\n",
        "# local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "# pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "pre_trained_model.trainable = True\n",
        "\n",
        "# Let's take a look to see how many layers are in the pretrained model\n",
        "nbr_layers_pretrained_model = len(pre_trained_model.layers)\n",
        "print(\"Nombre de couches dans le modèle pré-entraîné d'origine: \", nbr_layers_pretrained_model)\n",
        "\n",
        "# How to get the layer index from the layer name\n",
        "# https://www.thetopsites.net/article/50151157.shtml\n",
        "layer_names = [layer.name for layer in pre_trained_model.layers]\n",
        "last_layer_name = layer_names[-1]\n",
        "print(\"Nom de la dernière couche du modèle pré-entraîné complet:\",last_layer_name)\n",
        "last_layer_index = layer_names.index(last_layer_name)\n",
        "# Choix d'une nouvelle couche de sortie par essai / erreur\n",
        "last_layer_name = 'mixed5'\n",
        "print(\"Choix empirique de la dernière du modèle pré-entraîné:\",last_layer_name)\n",
        "last_layer_index = layer_names.index(last_layer_name)\n",
        "print(\"Index de la dernière couche du modèle pré-entraîné:\",last_layer_index)\n",
        "# Choix de la dernière couche non-entraînable ou dernière couche « gelée »\n",
        "last_layer_frozen_name = 'mixed3'\n",
        "print(\"Choix empirique de la dernière couche non-entraînable:\",last_layer_frozen_name)\n",
        "last_layer_frozen_index = layer_names.index(last_layer_frozen_name)\n",
        "print(\"Index de la dernière couche non-entraînable:\",last_layer_frozen_index)\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = last_layer_frozen_index\n",
        "print(\"Nombre de couches non entraînables dans le modèle préentraîné: \", fine_tune_at )\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in pre_trained_model.layers[:fine_tune_at]:\n",
        "    layer.trainable =  False\n",
        "\n",
        "print(\"Nombre de couches entraînables dans le modèle préentraîné: \", last_layer_index-fine_tune_at)\n",
        "last_layer = pre_trained_model.get_layer(last_layer_name)\n",
        "print('Dimensions de la dernière couche: ', last_layer.output_shape)\n",
        "last_output = last_layer.output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYc_wvQcTk6l"
      },
      "outputs": [],
      "source": [
        "# Create new data input\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "#inputs = data_augmentation(inputs)  # Apply random data augmentation\n",
        "\n",
        "# Pre-trained Xception weights requires that input be scaled\n",
        "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
        "# outputs: `(inputs * scale) + offset`\n",
        "\n",
        "# data_augmentation = keras.Sequential(\n",
        "#     [layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1),]\n",
        "# )\n",
        "# inputs = data_augmentation(inputs)  # Apply random data augmentation\n",
        "\n",
        "scale_layer = keras.layers.Rescaling(scale=1/127.5,offset=-1)\n",
        "inputs = scale_layer(inputs)\n",
        "\n",
        "pre_trained_model.inputs = inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eeNLUIfCJQB"
      },
      "outputs": [],
      "source": [
        "# Flatten the output layer to 1 dimension\n",
        "outputs = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "outputs = layers.Dense(1024, activation='relu')(outputs)\n",
        "# Add a dropout rate of 0.5\n",
        "outputs = layers.Dropout(0.5)(outputs)      \n",
        "# # Add a bottleneck layer\n",
        "# outputs = layers.Dense(512, activation='relu')(outputs)\n",
        "# outputs = layers.Dropout(0.2)(outputs)  \n",
        "# # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "# outputs = layers.Dense(1024, activation='relu')(outputs)\n",
        "# outputs = layers.Dropout(0.2)(outputs)  \n",
        "# Add a final softmax layer for classification\n",
        "# *** IMPORTANT *** 23 classes\n",
        "number_of_target_class = 23\n",
        "outputs = layers.Dense(number_of_target_class, activation='softmax')(outputs)           \n",
        "\n",
        "transfer_model = Model(pre_trained_model.input, outputs) \n",
        "# transfer_model = Model(pre_trained_model.input, outputs)\n",
        "print(\"Number of layers in the learning transfer model: \", len(transfer_model.layers))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96gJIRE0CJQE",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "transfer_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZxGej8rCJQG"
      },
      "outputs": [],
      "source": [
        "len(transfer_model.trainable_variables)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHcwa9acCJQM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# According to tf.keras.utils.image_dataset_from_directory documentation, 'int' is the default label_mode\n",
        "# 'int': means that the labels are encoded as integers (e.g. for sparse_categorical_crossentropy loss)\n",
        "# So, by default, tf.keras.utils.image_dataset_from_directory will create a set of labels for the dataset \n",
        "# as integer that go from 1 to the number of classes in the dataset.\n",
        "# In this case, the model should be compiled with a 'sparse_categorical_crossentropy' loss.\n",
        "transfer_model.compile(optimizer = RMSprop(learning_rate=0.0001), \n",
        "              loss = 'sparse_categorical_crossentropy', \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "print(\"Modèle compilé!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXj44_jSCJQN"
      },
      "outputs": [],
      "source": [
        "TRAINING_DIR = \"/tmp/lab_id_arbres/train/\"\n",
        "VALIDATION_DIR = \"/tmp/lab_id_arbres/val\"\n",
        "TESTING_DIR = \"/tmp/lab_id_arbres/test/\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (150, 150)\n",
        "\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(TRAINING_DIR,\n",
        "                                                            shuffle=True,\n",
        "                                                            validation_split=0.2,\n",
        "                                                            subset='validation',\n",
        "                                                            seed=42,\n",
        "                                                            batch_size=BATCH_SIZE,\n",
        "                                                            image_size=IMG_SIZE)\n",
        "\n",
        "test_dataset = tf.keras.utils.image_dataset_from_directory(TESTING_DIR,\n",
        "                                                           shuffle=True,\n",
        "                                                           batch_size=BATCH_SIZE,\n",
        "                                                           image_size=IMG_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FJpsONdWvda"
      },
      "outputs": [],
      "source": [
        "print('Number of training batches: %d' % tf.data.experimental.cardinality(train_dataset))\n",
        "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaQ862pnCJQQ"
      },
      "outputs": [],
      "source": [
        "initial_epochs = 10\n",
        "fine_tune_epochs = 10\n",
        "total_epochs =  initial_epochs + fine_tune_epochs\n",
        "\n",
        "# Callbacks are passed to the model via the callbacks argument in fit, \n",
        "# which takes a list of callbacks. You can pass any number of callbacks.\n",
        "callbacks_list = [\n",
        "    # # Interrupts training when improvement stops\n",
        "    # keras.callbacks.EarlyStopping(\n",
        "    #     # Monitors the model’s validation accuracy\n",
        "    #     monitor='val_accuracy',\n",
        "    #     # Interrupts training when accuracy has stopped \n",
        "    #     # improving for more than one epoch (that is, two epochs)\n",
        "    #     patience=2,\n",
        "    # ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        # Monitors the model’s validation loss\n",
        "        monitor='loss',\n",
        "        # Divides the learning rate by 2 when triggered\n",
        "        factor=0.5,\n",
        "        # The callback is triggered after the validation loss \n",
        "        # has stopped improving for 1 epochs.\n",
        "        patience=1,\n",
        "    ) \n",
        "\n",
        "]\n",
        "history_fine = transfer_model.fit(train_dataset,\n",
        "                                  epochs = total_epochs,\n",
        "                                  callbacks=callbacks_list,\n",
        "                                  verbose = 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7kJQvyVCJQR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history_fine.history['accuracy']\n",
        "# val_acc = history_fine.history['val_accuracy']\n",
        "loss = history_fine.history['loss']\n",
        "# val_loss = history_fine.history['val_loss']\n",
        "epochs = range(len(acc))\n",
        "golden_number = 1.618\n",
        "height = 6\n",
        "length = int(golden_number * height)\n",
        "plt.figure(figsize=(length,height))\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "# plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy - augmentation on training data with dropout')\n",
        "plt.legend(loc=0)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = transfer_model.evaluate(test_dataset)\n",
        "print('Test accuracy :', accuracy)"
      ],
      "metadata": {
        "id": "-HG1NiuAp-SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvgAD9crCJQU"
      },
      "outputs": [],
      "source": [
        "print(\"Fin de l'exécution du carnet IPython\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L78cI0YoUUJ8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Id_arbre_reseau_convolutif-transfert_paufinage-colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}