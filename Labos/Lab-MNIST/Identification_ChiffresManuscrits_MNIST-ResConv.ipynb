{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6428aeb9",
   "metadata": {},
   "source": [
    "<a style=\"float:left;\" href=\"https://colab.research.google.com/github/ClaudeCoulombe/VIARENA/blob/master/Labos/Lab-MNIST/Identification_ChiffresManuscrits_MNIST-ResConv.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<br/>\n",
    "### Rappel - Fonctionnement d'un carnet web iPython\n",
    "\n",
    "* Pour exécuter le code contenu dans une cellule d'un carnet iPython, cliquez dans la cellule et faites (⇧↵, shift-enter);\n",
    "* Le code d'un carnet iPython s'exécute séquentiellement de haut en bas de la page. Souvent, l'importation d'une bibliothèque Python ou l'initialisation d'une variable est préalable à l'exécution d'une cellule située plus bas. Il est donc recommandé d'exécuter les cellules en séquence. Enfin, méfiez-vous des retours en arrière qui peuvent réinitialiser certaines variables;\n",
    "* Pour obtenir de l'information sur une fonction, utilisez la commande Python `help(`\"nom de la fonction\"`)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c12a2e",
   "metadata": {},
   "source": [
    "# Identification de chiffres manuscrits - jeu de données MNIST\n",
    "## Réseau convolutif - LeNet\n",
    "\n",
    "#### Inspiration: \n",
    " \n",
    "Richmond Alake - <a href=\"https://towardsdatascience.com/understanding-and-implementing-lenet-5-cnn-architecture-deep-learning-a2d531ebc342\" target='_blank'>Understanding and Implementing LeNet-5 CNN Architecture</a> - 25 juin 2020\n",
    "\n",
    "Yann LeCun, Léon Bottou, Yoshua Bengio et Patrick Haffner <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\" target='_blank'>Gradient-Based Learning Applied to Document Recognition</a> - 1998\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b819907f",
   "metadata": {},
   "source": [
    "## Importation des bibliothèques Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4235134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\",tf.__version__)\n",
    "from tensorflow import keras\n",
    "print(\"Keras version:\",keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edea099",
   "metadata": {},
   "source": [
    "## Jeu de données - chiffres manuscrits MNIST\n",
    "\n",
    "Le jeu de données MNIST (Modified National Institute of Standards and Technology) comporte 60,000 images en tons de gris de 28×28 pixels de chiffres manuscrits étiquetés de 0 à 9. Site web: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Il est incorporé dans keras.datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc7fec",
   "metadata": {},
   "source": [
    "### Lecture des données\n",
    "### Séparation entre jeu de données d'entraînement et jeux de données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# le jeu de données MNIST\n",
    "from keras.datasets import mnist\n",
    "\n",
    "dic_noms_classe = { \n",
    "    0 : \"0\",\n",
    "    1 : \"1\",\n",
    "    2 : \"2\",\n",
    "    3 : \"3\",\n",
    "    4 : \"4\",\n",
    "    5 : \"5\",\n",
    "    6 : \"6\",\n",
    "    7 : \"7\",\n",
    "    8 : \"8\",\n",
    "    9 : \"9\",\n",
    "}\n",
    "\n",
    "# lire le jeu de données MNIST et le diviser entre\n",
    "# les données d'entrainement et les données de test\n",
    "# MNIST est déjà divisé en un jeu de données d'entraînement (les 60 000 premières images) \n",
    "# et un jeu de données de test (les 10 000 dernières images).\n",
    "(attributs_entrainement, classes_cibles_entrainement), (attributs_test, classes_cibles_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d72b18c",
   "metadata": {},
   "source": [
    "## Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5eb0f8",
   "metadata": {},
   "source": [
    "### Portrait des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cb8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portrait des données \n",
    "print()\n",
    "print('Entraînement: attributs=%s, classes=%s' % (attributs_entrainement.shape, classes_cibles_entrainement.shape))\n",
    "print('Test: attributs=%s, classes=%s' % (attributs_test.shape, classes_cibles_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributs_entrainement[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0377c9d",
   "metadata": {},
   "source": [
    "### Visualisation de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ca8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les 24 premières images\n",
    "print()\n",
    "print(\"Quelques images avec leur étiquette de classe-cible...\")\n",
    "%matplotlib inline\n",
    "# définir subplot\n",
    "fig, axes = plt.subplots(nrows=4,ncols=6,figsize=(13,10))\n",
    "for i_rangee in range(0,4):\n",
    "    for i_colonne in range(0,6):\n",
    "        axes[i_rangee,i_colonne].set_title(dic_noms_classe[int(classes_cibles_entrainement[i_rangee*6+i_colonne])],\n",
    "                                           fontsize=10)\n",
    "        axes[i_rangee,i_colonne].imshow(attributs_entrainement[i_rangee*6+i_colonne],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22f32c",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41603a9",
   "metadata": {},
   "source": [
    "### Conversion des étiquettes-cibles en vecteurs binaires à un bit discriminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16cc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des étiquettes-cibles en vecteurs binaires à un bit discriminant\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "classes_cibles_entrainement = to_categorical(classes_cibles_entrainement)\n",
    "classes_cibles_test = to_categorical(classes_cibles_test)\n",
    "print(\"Conversion des étiquettes-cibles en vecteurs binaires terminée!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269ae81",
   "metadata": {},
   "source": [
    "### Normalisation des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cb1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "def normalisation(entrainement, test):\n",
    "    # convertir de nombres entiers à nombres décimaux\n",
    "    entrainement_normalise = entrainement.astype('float32')\n",
    "    test_normalise = test.astype('float32')\n",
    "    # normalisation à un nombre entre 0 et 1\n",
    "    entrainement_normalise = entrainement_normalise / 255.0\n",
    "    test_normalise = test_normalise / 255.0\n",
    "    return entrainement_normalise, test_normalise\n",
    "\n",
    "attributs_entrainement, attributs_test = normalisation(attributs_entrainement, attributs_test)\n",
    "\n",
    "print(\"Normalisation des images terminée!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933c907",
   "metadata": {},
   "source": [
    "## Construction d'un réseau convolutif - LeNet\n",
    "\n",
    "<img src=\"https://cours.edulib.org/asset-v1:Cegep-Matane+VAERN.1FR+P2021+type@asset+block@LeNet-52.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334320dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du modèle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "\n",
    "print(\"Création d'un modèle LeNet...\")\n",
    "\n",
    "# dimension_entree = (nombre_attributs,)\n",
    "nombre_classes_cibles = 10\n",
    "\n",
    "increment_convolution = (1,1)\n",
    "increment_echantillonnage = (2,2)\n",
    "\n",
    "leNet = Sequential()\n",
    "\n",
    "# Apprentissage et extraction des attributs\n",
    "# C1\n",
    "leNet.add(Conv2D(filters=6, \n",
    "                 kernel_size=(5,5), \n",
    "                 strides = increment_convolution,\n",
    "                 padding = 'same',\n",
    "                 activation='tanh', \n",
    "                 input_shape=(28,28,1)))\n",
    "# S2\n",
    "leNet.add(AveragePooling2D(pool_size=(2, 2),\n",
    "                           strides=increment_echantillonnage))\n",
    "\n",
    "# C3\n",
    "leNet.add(Conv2D(filters=16, \n",
    "                 kernel_size=(5, 5),\n",
    "                 strides = increment_convolution,\n",
    "                 padding = 'valid',\n",
    "                 activation='tanh'))\n",
    "# S4\n",
    "leNet.add(AveragePooling2D(pool_size=(2, 2),\n",
    "                           strides=increment_echantillonnage))\n",
    "\n",
    "# Classification des images\n",
    "leNet.add(Flatten())\n",
    "\n",
    "# C5\n",
    "leNet.add(Dense(units=120, \n",
    "                activation='tanh'))\n",
    "# F6\n",
    "leNet.add(Dense(units=84, \n",
    "                activation='relu'))\n",
    "\n",
    "# Couche de sortie\n",
    "leNet.add(Dense(units=nombre_classes_cibles,\n",
    "                activation = 'softmax'))\n",
    "\n",
    "print()\n",
    "print(\"Description du modèle de base:\")\n",
    "leNet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8ffd8",
   "metadata": {},
   "source": [
    "### Compilation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3672c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Compilation du modèle...\")\n",
    "\n",
    "leNet.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Modèle compilé!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d75a67",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ee15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle\n",
    "\n",
    "print()\n",
    "print(\"Entraînement du modèle...\")\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "taille_jeu_validation = 0.1\n",
    "\n",
    "# Les images MNIST sont de 28x28 pixels, ce qui est plus petit que ce que LeNet-5 \n",
    "# attend de 32x32 pixels. Une solution simple consiste simplement à remplir les images \n",
    "# avec des zéros pour amener la taille des images MNIST jusqu'à 32x32 pixels.\n",
    "\n",
    "attributs_entrainement = attributs_entrainement.reshape(attributs_entrainement.shape[0], 28, 28, 1)\n",
    "attributs_test = attributs_test.reshape(attributs_test.shape[0], 28, 28, 1)\n",
    "\n",
    "traces_entrainement = leNet.fit(attributs_entrainement,\n",
    "                                classes_cibles_entrainement,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs,\n",
    "                                validation_split=taille_jeu_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff02d0",
   "metadata": {},
   "source": [
    "## Évaluation du modèle avec un jeu de données test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40319bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation du modèle\n",
    "\n",
    "print()\n",
    "print(\"Évaluation du modèle...\")\n",
    "\n",
    "resultats = leNet.evaluate(attributs_test, classes_cibles_test, verbose=0)\n",
    "print(\"Exactitude test: {:.2f}%\".format(resultats[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7b6a55",
   "metadata": {},
   "source": [
    "**Note:** On constate un gain d'environ 5% avec l'utilisation d'un réseau convolutif comme l'antique LeNet et l'exactitude obtenue avec un perceptron multicouche. Cette différence démontre l'avantage des réseaux convolutifs pour la reconnaissance d'images même simples comme des chiffres manuscrits en noir et blanc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84499d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exécution carnet IPython terminée!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0b1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
