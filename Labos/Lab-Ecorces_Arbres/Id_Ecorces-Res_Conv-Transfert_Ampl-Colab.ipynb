{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa851b90",
      "metadata": {
        "id": "aa851b90"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClaudeCoulombe/VIARENA/blob/master/Labos/Lab-Ecorces_Arbres/Id_Ecorces-Res_Conv-Transfert_Ampl-Colab.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "### Rappel - Fonctionnement d'un carnet web iPython\n",
        "\n",
        "* Pour exécuter le code contenu dans une cellule d'un carnet iPython, cliquez dans la cellule et faites (⇧↵, shift-enter) \n",
        "* Le code d'un carnet iPython s'exécute séquentiellement de haut en bas de la page. Souvent, l'importation d'une bibliothèque Python ou l'initialisation d'une variable est préalable à l'exécution d'une cellule située plus bas. Il est donc recommandé d'exécuter les cellules en séquence. Enfin, méfiez-vous des retours en arrière qui peuvent réinitialiser certaines variables."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e369a6",
      "metadata": {
        "id": "62e369a6"
      },
      "source": [
        "# Identification d'arbres à partir de leur écorce\n",
        "## Réseau convolutif, apprentissage par transfert et amplification des données"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21790c87",
      "metadata": {
        "id": "21790c87"
      },
      "source": [
        "### Inspiration\n",
        "\n",
        "Ce laboratoire s'inspire de plusieurs sources en logiciels libres dont voici les principales.\n",
        "\n",
        "#### Transfer learning and fine-tuning - site Google / Tutoriels TensorFlow\n",
        "https://www.tensorflow.org/tutorials/images/transfer_learning\n",
        "\n",
        "#### Data augmentation  - site Google / Tutoriels TensorFlow\n",
        "https://www.tensorflow.org/tutorials/images/data_augmentation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f279ee",
      "metadata": {
        "id": "46f279ee"
      },
      "outputs": [],
      "source": [
        "# MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet                                                                                                                    # IGNORE_COPYRIGHT: cleared by OSS licensing\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6ce4f4d",
      "metadata": {
        "id": "d6ce4f4d"
      },
      "source": [
        "# Apprentissage par transfert & amplification des données"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f343d37",
      "metadata": {
        "id": "8f343d37"
      },
      "source": [
        "## Fixer le hasard pour la reproductibilité\n",
        "\n",
        "La mise au point de réseaux de neurones implique certains processus aléatoires. Afin de pouvoir reproduire et comparer vos résultats d'expérience, vous fixez temporairement l'état aléatoire grâce à un germe aléatoire unique.\n",
        "\n",
        "Pendant la mise au point, vous fixez temporairement l'état aléatoire pour la reproductibilité mais vous répétez l'expérience avec différents germes ou états aléatoires et prenez la moyenne des résultats.\n",
        "<br/>\n",
        "\n",
        "**Note** : Pour un système en production, vous ravivez simplement l'état  purement aléatoire avec l'instruction `GERME_ALEATOIRE = None`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba6a5cb",
      "metadata": {
        "id": "8ba6a5cb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Définir un germe aléatoire\n",
        "GERME_ALEATOIRE = 1\n",
        "\n",
        "# Définir un état aléatoire pour Python\n",
        "os.environ['PYTHONHASHSEED'] = str(GERME_ALEATOIRE)\n",
        "\n",
        "# Définir un état aléatoire pour Python random\n",
        "import random\n",
        "random.seed(GERME_ALEATOIRE)\n",
        "\n",
        "# Définir un état aléatoire pour NumPy\n",
        "import numpy as np\n",
        "np.random.seed(GERME_ALEATOIRE)\n",
        "\n",
        "# Définir un état aléatoire pour TensorFlow\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(GERME_ALEATOIRE)\n",
        "\n",
        "# Note: Retrait du comportement déterministe\n",
        "# à cause de keras.layers.RandomContrast(...)\n",
        "# dont il n'existe pas de version déterministe\n",
        "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "# os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "print(\"Germe aléatoire fixé\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "476606c3",
      "metadata": {
        "id": "476606c3"
      },
      "source": [
        "## Acquisition des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f261ef2",
      "metadata": {
        "id": "3f261ef2"
      },
      "outputs": [],
      "source": [
        "# Notez que nous n'avons pas inclus Acer platanoides (2), Pinus rigida (15) et Populus grandidentata (18) \n",
        "# car il n'y a pas suffisamment d'images dans ces catégories pour obtenir des résultats significatifs.\n",
        "\n",
        "data_ecorces = {\n",
        "    'SAB': 1,  \n",
        "#    'ERB': 2,  # Pas assez de spécimens - seulement 1\n",
        "    'ERR': 3, \n",
        "    'ERS': 4, \n",
        "    'BOJ': 5, \n",
        "    'BOP': 6,\n",
        "    'HEG': 7,  \n",
        "    'FRA': 8, \n",
        "    'MEL': 9,  \n",
        "    'OSV': 10, \n",
        "    'EPO': 11,\n",
        "    'EPB': 12,\n",
        "    'EPN': 13,\n",
        "    'EPR': 14,\n",
        "#    'PID': 15, # Pas assez de spécimens - seulement 4\n",
        "    'PIR': 16, \n",
        "    'PIB': 17, \n",
        "#    'PEG': 18, # Pas assez de spécimens - seulement 3\n",
        "    'PET': 19, \n",
        "    'CHR': 20,\n",
        "    'THO': 21, \n",
        "    'PRU': 22, \n",
        "    'ORA': 23  \n",
        "}\n",
        "\n",
        "noms_arbres = {\n",
        "            1: '\\emph{Abies balsamea} - Sapin Baumier - Balsam fir',\n",
        "            2: '\\emph{Acer platanoides} - Érable de Norvège - Norway maple',\n",
        "            3: '\\emph{Acer rubrum} - Érable rouge - Red maple',\n",
        "            4: '\\emph{Acer saccharum} - Érable à sucre - Sugar maple',\n",
        "            5: '\\emph{Betula alleghaniensis} - Bouleau jaune - Yellow birch',\n",
        "            6: '\\emph{Betula papyrifera} - Bouleau à papier - White birch',\n",
        "            7: '\\emph{Fagus grandifolia} - Hêtre à grandes feuilles - American beech',\n",
        "            8: \"\\emph{Fraxinus americana} - Frêne d'Amérique - White ash\",\n",
        "            9: '\\emph{Larix laricina} - Mélèze - Tamarack',\n",
        "            10: '\\emph{Ostrya virginiana} - Ostryer de Virginie - American hophornbeam',\n",
        "            11: '\\emph{Picea abies} - Épinette de Norvège - Norway spruce',\n",
        "            12: '\\emph{Picea glauca} - Épinette blanche - White spruce',\n",
        "            13: '\\emph{Picea mariana} - Épinette noire - Black spruce',\n",
        "            14: '\\emph{Picea rubens} - Épinette rouge - Red spruce',\n",
        "            15: '\\emph{Pinus rigida} - Pin rigide - Pitch pine',\n",
        "            16: '\\emph{Pinus resinosa} - Pin rouge - Red pine',\n",
        "            17: '\\emph{Pinus strobus} - Pin blanc - Eastern white pine',\n",
        "            18: '\\emph{Populus grandidentata} - Peuplier à grandes dents - Big-tooth aspen',\n",
        "            19: '\\emph{Populus tremuloides} - Peuplier faux tremble - Quaking aspen',\n",
        "            20: '\\emph{Quercus rubra} - Chêne rouge - Northern red oak',\n",
        "            21: '\\emph{Thuja occidentalis} - Thuya occidental - Northern white cedar',\n",
        "            22: '\\emph{Tsuga canadensis} - Pruche du Canada - Eastern Hemlock',\n",
        "            23: \"\\emph{Ulmus americana} - Orme d'Amérique - American elm\"\n",
        "        }\n",
        "\n",
        "dict_no_arbres_ID = {\n",
        "    '1':'SAB',  \n",
        "    '2':'ERB',\n",
        "    '3':'ERR', \n",
        "    '4':'ERS', \n",
        "    '5':'BOJ', \n",
        "    '6':'BOP',\n",
        "    '7':'HEG',  \n",
        "    '8':'FRA', \n",
        "    '9':'MEL',  \n",
        "    '10':'OSV', \n",
        "    '11':'EPO',\n",
        "    '12':'EPB',\n",
        "    '13':'EPN',\n",
        "    '14':'EPR',\n",
        "    '15':'PID',\n",
        "    '16':'PIR', \n",
        "    '17':'PIB',\n",
        "    '18':'PEG',\n",
        "    '19':'PET', \n",
        "    '20':'CHR',\n",
        "    '21':'THO', \n",
        "    '22':'PRU', \n",
        "    '23':'ORA'  \n",
        "}\n",
        "\n",
        "print(\"Code exécuté\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a8ad21f",
      "metadata": {
        "id": "1a8ad21f"
      },
      "outputs": [],
      "source": [
        "# Dictionnaire Python des URL qui pointent vers des données sur Google Doc\n",
        "\n",
        "data_zip_urls_dict = {\n",
        "   \"BOJ\":\"https://drive.google.com/file/d/1d2zxg2pt5S8UJIK-E7IuWfGN0d1kxxMw/view?usp=sharing\",\n",
        "   \"BOP\":\"https://drive.google.com/file/d/12cg6UO4HLnjk5fE_KXtrgdC2s8uGh4Zp/view?usp=sharing\",\n",
        "   \"CHR\":\"https://drive.google.com/file/d/1Nq19-I-Q577KXMTFrkhlJDhMfclh0cWn/view?usp=sharing\",\n",
        "   \"EPB\":\"https://drive.google.com/file/d/1K_Ncw8VEiuDZ_iJDbYToMq-GO5dzKHns/view?usp=sharing\",\n",
        "   \"EPN\":\"https://drive.google.com/file/d/1S309DYmg76SrIA89aVQWXCMwm6CzhN8b/view?usp=sharing\",\n",
        "   \"EPO\":\"https://drive.google.com/file/d/1fTKEcpYgmRg4spUpcH0FAiAnoRgANafL/view?usp=sharing\",\n",
        "   \"EPR\":\"https://drive.google.com/file/d/1qRhtZ8LZjH_45fxetG7swg3ok3znk8CJ/view?usp=sharing\",\n",
        "#   \"ERB\":\"https://drive.google.com/file/d/1ighbGniKAT_GrPm4RtsIAuN1STg9sjR9/view?usp=sharing\", # Assez de données?\n",
        "   \"ERR\":\"https://drive.google.com/file/d/1rEo1thMNJTgFeTzTOfI11_FPSqMgbHSL/view?usp=sharing\",\n",
        "   \"ERS\":\"https://drive.google.com/file/d/1ts-t7bOH9DfKj0q0v35nMgKHgVT0ZjyG/view?usp=sharing\",\n",
        "   \"FRA\":\"https://drive.google.com/file/d/1yLacRGW7JtlFWV5asEXHpAToClL38D64/view?usp=sharing\",\n",
        "   \"HEG\":\"https://drive.google.com/file/d/1zoJKEIrsCD1XxglgPJkEygumev1xRQ3U/view?usp=sharing\",\n",
        "   \"MEL\":\"https://drive.google.com/file/d/1Wdy3DDnWfUysXjcIFFq12UFW7tlTYDT2/view?usp=sharing\",\n",
        "   \"ORA\":\"https://drive.google.com/file/d/19_oYwCAaPfP6vMuqUnAzIQAa39Brxhfi/view?usp=sharing\",\n",
        "   \"OSV\":\"https://drive.google.com/file/d/1VJCCZN1iwBK2Nzh_PHC9xvw63xiLuXXI/view?usp=sharing\",\n",
        "#   \"PEG\":\"https://drive.google.com/file/d/1YUWH4IaTnmcoIAavZq8HyXByJxO7_zBg/view?usp=sharing\", # Assez de données?\n",
        "   \"PET\":\"https://drive.google.com/file/d/13bMkvr_1mRz1TuOcX8-c-LfTSIsNKrve/view?usp=sharing\",\n",
        "   \"PIB\":\"https://drive.google.com/file/d/17J9g1xm6-ji52k2pgJr7mUrJdS1ASSqP/view?usp=sharing\",\n",
        "#   \"PID\":\"https://drive.google.com/file/d/12xswrf4pDmTAcYZDAY9D-0HniLjGJCxp/view?usp=sharing\", # Assez de données?\n",
        "   \"PIR\":\"https://drive.google.com/file/d/1qny4meuoT-HYZ_KTyPQbQnzLhebkgkfU/view?usp=sharing\",\n",
        "   \"PRU\":\"https://drive.google.com/file/d/1xQWHQvIbwRRBoi2F27q22_drUeM8m3S8/view?usp=sharing\",\n",
        "   \"SAB\":\"https://drive.google.com/file/d/1ol2mlYAz5bMfQkwqcnxhCOg4avftYtRe/view?usp=sharing\",\n",
        "   \"THO\":\"https://drive.google.com/file/d/1_mI0saGpfxb4wnhElCzxg0WU4OiFHkfP/view?usp=sharing\",\n",
        "  \n",
        "}\n",
        "data_zip_urls_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec086bb",
      "metadata": {
        "id": "bec086bb"
      },
      "outputs": [],
      "source": [
        "# Création des répertoires de données\n",
        "# Nous allons créer un répertoire de base `data` et des répertoires pour les données \n",
        "# d'entrainement, de validation et de test pour chaque étiquette cible\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "    os.mkdir(\"/content/data/\")\n",
        "except OSError:\n",
        "    pass\n",
        "try:\n",
        "    os.mkdir(\"/content/lab_ecorces/\")\n",
        "except OSError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "055b0b83",
      "metadata": {
        "id": "055b0b83"
      },
      "outputs": [],
      "source": [
        "# Demande d'autorisation pour télécharger les données sur Google Drive \n",
        "# Référence: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "import shutil\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18140a7a",
      "metadata": {
        "id": "18140a7a"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b900d0",
      "metadata": {
        "id": "02b900d0"
      },
      "outputs": [],
      "source": [
        "nombre_classes = 0\n",
        "for arbre_id in data_zip_urls_dict.keys():\n",
        "    url = data_zip_urls_dict[arbre_id]\n",
        "    id_fichier = url.split('/')[5]\n",
        "    fichier = drive.CreateFile({'id':id_fichier})\n",
        "    nom_fichier = arbre_id + \".zip\"\n",
        "    # télécharger le fichier nom_fichier\n",
        "    fichier.GetContentFile(\"/content/data/\" + nom_fichier)\n",
        "    print(\"Fichier \" + nom_fichier + \" téléchargé\")\n",
        "    zip_ref = zipfile.ZipFile(\"/content/data/\" + nom_fichier, 'r')\n",
        "    zip_ref.extractall(\"/content/data\")\n",
        "    zip_ref.close()\n",
        "    print(\"Fichier \" + nom_fichier + \" décompressé\")\n",
        "    try:\n",
        "        os.remove(\"/content/data/\"+nom_fichier)\n",
        "        print(\"Fichier \" + nom_fichier + \" effacé\")\n",
        "    except:\n",
        "        print(\"?\")\n",
        "    nombre_classes += 1\n",
        "shutil.rmtree('/content/data/__MACOSX')\n",
        "print(\"nombre_classes:\",nombre_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "517a8af2",
      "metadata": {
        "id": "517a8af2"
      },
      "source": [
        "### Répartition des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "591ee2b2",
      "metadata": {
        "id": "591ee2b2"
      },
      "outputs": [],
      "source": [
        "# Installation des bibliothèques Python `split-folders` et `tqdm`\n",
        "!pip3 install split-folders tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "085f7b38",
      "metadata": {
        "id": "085f7b38"
      },
      "outputs": [],
      "source": [
        "# Répartition des données d'entraînement, de validation et de tests\n",
        "import splitfolders\n",
        "import pathlib\n",
        "\n",
        "#### répertoire des données en entrée et des données une fois réparties\n",
        "repertoire_entree = \"/content/data\"\n",
        "repertoire_donnees_reparties = \"/content/lab_ecorces\"\n",
        "# => train, val, test\n",
        "\n",
        "nombre_images = len(list(pathlib.Path(repertoire_entree).glob('*/*.jpg')))\n",
        "print(\"Nombre total d'images:\",nombre_images)\n",
        "\n",
        "splitfolders.ratio(repertoire_entree, output=repertoire_donnees_reparties, seed=42, ratio = (0.80, 0.15, 0.05))\n",
        "\n",
        "print(\"\\nRépartition des données terminée!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c3026e",
      "metadata": {
        "id": "66c3026e"
      },
      "source": [
        "### Visualisation d'un échantillon des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b725cd3",
      "metadata": {
        "id": "8b725cd3"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pic_index = 4\n",
        "\n",
        "REPERTOIRE_ENTRAINEMENT = \"/content/lab_ecorces/train/\"\n",
        "\n",
        "for arbre_id in data_ecorces.keys():\n",
        "    try:\n",
        "        dir_path = os.path.join(TRAINING_DIR,arbre_id+os.sep)\n",
        "        liste_fichiers = os.listdir(os.path.join(TRAINING_DIR,arbre_id+os.sep))\n",
        "        next_two_pics = [os.path.join(dir_path, fname) for fname in liste_fichiers[0:pic_index]]\n",
        "        fig = plt.figure(figsize=(12,4))\n",
        "        print(\"_\"*90)\n",
        "        print(arbre_id,noms_arbres[data_ecorces[arbre_id]].split('-')[1])\n",
        "        for i, img_path in enumerate(next_two_pics):\n",
        "            print(img_path)\n",
        "            img = mpimg.imread(img_path)\n",
        "            plt.subplot(1,pic_index,i+1)\n",
        "            plt.imshow(img)\n",
        "            plt.axis('Off')\n",
        "        plt.show()\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1800e2ef",
      "metadata": {
        "id": "1800e2ef"
      },
      "source": [
        "### Création de flux de lots de données\n",
        "\n",
        "Chargeons ces images en mémoire en créant des `tf.data.dataset` à l'aide de l'utilitaire `tf.keras.utils.image_dataset_from_directory`. \n",
        "\n",
        "tf.data.Dataset prend en charge l'écriture de chaîne de traitement d'entrée de données efficaces. L'itération se produit dans un flux continu, de sorte que l'ensemble de données complet n'a pas besoin de tenir dans la mémoire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e689369",
      "metadata": {
        "id": "9e689369"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "print(\"Version de Keras:\",keras.__version__)\n",
        "import tensorflow as tf\n",
        "print(\"Version de TensorFlow :\",tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1081f919",
      "metadata": {
        "id": "1081f919"
      },
      "outputs": [],
      "source": [
        "REPERTOIRE_ENTRAINEMENT = \"/content/lab_ecorces/train/\"\n",
        "REPERTOIRE_VALIDATION = \"/content/lab_ecorces/val\"\n",
        "REPERTOIRE_TEST = \"/content/lab_ecorces/test/\"\n",
        "\n",
        "TAILLE_LOT = 32\n",
        "HAUTEUR_IMAGE = 150\n",
        "LARGEUR_IMAGE = 150\n",
        "TAILLE_IMAGE = (HAUTEUR_IMAGE, LARGEUR_IMAGE)\n",
        "NOMBRE_CANAUX = 3\n",
        "\n",
        "donnees_entrainement = tf.keras.utils.image_dataset_from_directory(REPERTOIRE_ENTRAINEMENT,\n",
        "                                                                   batch_size=TAILLE_LOT,\n",
        "                                                                   image_size=TAILLE_IMAGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6555acc5",
      "metadata": {
        "id": "6555acc5"
      },
      "outputs": [],
      "source": [
        "print(\"Type Python de donnees_entrainement):\",type(donnees_entrainement))\n",
        "flux_images, flux_etiquettes = next(iter(donnees_entrainement))\n",
        "print(\"Dimensions du flux d'images:\",flux_images.shape)\n",
        "print(\"Nombre d'images):\",len(flux_images))\n",
        "print(\"Dimensions du flux d'étiquettes:\",flux_etiquettes.shape)\n",
        "print(\"Nombre d'étiquettes):\",len(flux_etiquettes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d3a8f25",
      "metadata": {
        "id": "8d3a8f25"
      },
      "outputs": [],
      "source": [
        "liste_noms_classes = donnees_entrainement.class_names\n",
        "print(liste_noms_classes)\n",
        "print(len(liste_noms_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a34373",
      "metadata": {
        "id": "76a34373"
      },
      "outputs": [],
      "source": [
        "donnees_validation = tf.keras.utils.image_dataset_from_directory(REPERTOIRE_VALIDATION,\n",
        "                                                                 batch_size=TAILLE_LOT,\n",
        "                                                                 image_size=TAILLE_IMAGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbccd7be",
      "metadata": {
        "id": "bbccd7be"
      },
      "outputs": [],
      "source": [
        "donnees_test = tf.keras.utils.image_dataset_from_directory(REPERTOIRE_TEST,\n",
        "                                                           batch_size=TAILLE_LOT,\n",
        "                                                           image_size=TAILLE_IMAGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70fb9317",
      "metadata": {
        "id": "70fb9317"
      },
      "outputs": [],
      "source": [
        "print(\"Nombre de lots de données d'entraînement: %d\" % tf.data.experimental.cardinality(donnees_entrainement))\n",
        "print(\"Nombre de lots de données de validation: %d\" % tf.data.experimental.cardinality(donnees_validation))\n",
        "print('Nombre de lots de données de test: %d' % tf.data.experimental.cardinality(donnees_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46144d64",
      "metadata": {
        "id": "46144d64"
      },
      "source": [
        "## Prétraitement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1858b28b",
      "metadata": {
        "id": "1858b28b"
      },
      "source": [
        "### Amplification des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5074a5b5",
      "metadata": {
        "id": "5074a5b5"
      },
      "outputs": [],
      "source": [
        "couches_amplification = tf.keras.Sequential([\n",
        "    keras.layers.RandomFlip(\"horizontal\"),\n",
        "    keras.layers.RandomFlip(\"vertical\"),\n",
        "    keras.layers.RandomRotation(0.1),\n",
        "    keras.layers.RandomZoom(0.3),\n",
        "    keras.layers.RandomContrast(0.3),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2020357",
      "metadata": {
        "id": "a2020357"
      },
      "source": [
        "### Normalisation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6fa76b8",
      "metadata": {
        "id": "d6fa76b8"
      },
      "outputs": [],
      "source": [
        "couches_normalisation = keras.Sequential([\n",
        "    keras.layers.Resizing(HAUTEUR_IMAGE,LARGEUR_IMAGE),\n",
        "    keras.layers.Rescaling(1./255)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0775da7c",
      "metadata": {
        "id": "0775da7c"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def pretraitement(jeu_donnees, melanger=False, amplifier=False):\n",
        "\n",
        "    if melanger:\n",
        "        jeu_donnees = jeu_donnees.shuffle(1000)\n",
        "        \n",
        "    # Amplifier seulement les données d'entraînement\n",
        "    if amplifier:\n",
        "        jeu_donnees = jeu_donnees.map(lambda x, y: (couches_amplification(x,training=True), y),\n",
        "                                      num_parallel_calls=AUTOTUNE\n",
        "                                     )\n",
        "        \n",
        "    # Normaliser les jeux de données\n",
        "    jeu_donnees = jeu_donnees.map(lambda x, y: (couches_normalisation(x), y),\n",
        "                                  num_parallel_calls=AUTOTUNE\n",
        "                                 )\n",
        "    \n",
        "    # Utiliser des tampons de préextraction sur tous les jeux de données\n",
        "    return jeu_donnees.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"Fonction de prétraitement prête!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2395892b",
      "metadata": {
        "id": "2395892b"
      },
      "source": [
        "#### Visualisation d'un échantillon de données amplifiées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f6a327",
      "metadata": {
        "id": "22f6a327"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 10))\n",
        "for images, _ in donnees_entrainement.take(1):\n",
        "    fig.clear()\n",
        "    for i in range(9):\n",
        "        images_augmentees = couches_amplification(images)\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images_augmentees[0].numpy().astype(\"uint8\"))\n",
        "        # plt.imshow(augmented_images[0], cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32a723ef",
      "metadata": {
        "id": "32a723ef"
      },
      "source": [
        "#### Normalisation des données d'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3356e089",
      "metadata": {
        "id": "3356e089"
      },
      "outputs": [],
      "source": [
        "print(\"Type Python de donnees_entrainement):\",type(donnees_entrainement))\n",
        "donnees_entrainement_normalisees = pretraitement(donnees_entrainement,\n",
        "                                                 melanger=True,\n",
        "                                                 amplifier=True)\n",
        "print(\"Type Python donnees_entrainement_normalisees):\",type(donnees_entrainement_normalisees))\n",
        "lot_images, lot_etiquettes = next(iter(donnees_entrainement_normalisees))\n",
        "premiere_image = lot_images[0]\n",
        "# Notez que les valeurs des pixels sont maintenant dans l'intervalle `[0,1]`\n",
        "print(np.min(premiere_image), np.max(premiere_image))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc2ed11d",
      "metadata": {
        "id": "fc2ed11d"
      },
      "outputs": [],
      "source": [
        "images, etiquettes = next(iter(donnees_entrainement_normalisees))\n",
        "print(\"Dimensions flux d'images:\",images.shape)\n",
        "print(\"Nombre d'images:\",len(images))\n",
        "print(\"Dimensions flux d'étiquette:\",etiquettes.shape)\n",
        "print(\"Nombre d'étiquettes:\",len(images))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7f057ca",
      "metadata": {
        "id": "b7f057ca"
      },
      "source": [
        "#### Normalisation des données de validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf7a402",
      "metadata": {
        "id": "bdf7a402"
      },
      "outputs": [],
      "source": [
        "print(\"Type Python de donnees_validation):\",type(donnees_validation))\n",
        "donnees_validation_normalisees = pretraitement(donnees_validation,\n",
        "                                               melanger=False,\n",
        "                                               amplifier=False)\n",
        "print(\"Type Python donnees_validation_normalisees):\",type(donnees_validation_normalisees))\n",
        "lot_images, lot_etiquettes = next(iter(donnees_validation_normalisees))\n",
        "premiere_image = lot_images[0]\n",
        "# Notez que les valeurs des pixels sont maintenant dans l'intervalle `[0,1]`\n",
        "print(np.min(premiere_image), np.max(premiere_image))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "420871f8",
      "metadata": {
        "id": "420871f8"
      },
      "outputs": [],
      "source": [
        "images, etiquettes = next(iter(donnees_validation_normalisees))\n",
        "print(\"Dimensions flux d'images:\",images.shape)\n",
        "print(\"Nombre d'images:\",len(images))\n",
        "print(\"Dimensions flux d'étiquette:\",etiquettes.shape)\n",
        "print(\"Nombre d'étiquettes:\",len(images))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c51d7ab5",
      "metadata": {
        "id": "c51d7ab5"
      },
      "source": [
        "#### Normalisation des données de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b26ef0",
      "metadata": {
        "id": "17b26ef0"
      },
      "outputs": [],
      "source": [
        "print(\"Type Python de donnees_test):\",type(donnees_test))\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "donnees_test = donnees_test.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(\"Type Python de donnees_test):\",type(donnees_test))\n",
        "donnees_test_normalisees = donnees_test.map(lambda x, y: (couches_normalisation(x), y))\n",
        "print(\"Type Python de donnees_test_normalisees:\",type(donnees_test_normalisees))\n",
        "lot_images, lot_etiquettes = next(iter(donnees_test_normalisees))\n",
        "premiere_image = lot_images[0]\n",
        "# Notez que les valeurs des pixels sont maintenant dans l'intervalle `[0,1]`\n",
        "print(np.min(premiere_image), np.max(premiere_image))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a36ad0",
      "metadata": {
        "id": "d8a36ad0"
      },
      "outputs": [],
      "source": [
        "images, etiquettes = next(iter(donnees_test_normalisees))\n",
        "print(\"Dimensions flux d'images:\",images.shape)\n",
        "print(\"Nombre d'images:\",len(images))\n",
        "print(\"Dimensions flux d'étiquette:\",etiquettes.shape)\n",
        "print(\"Nombre d'étiquettes:\",len(images))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da9b0690",
      "metadata": {
        "id": "da9b0690"
      },
      "source": [
        "## Création d'un modèle d'apprentissage par transfert"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccbf754e",
      "metadata": {
        "id": "ccbf754e"
      },
      "source": [
        "### Importation d'un modèle inception pré-entraîné sur ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b474fd",
      "metadata": {
        "id": "84b474fd"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b73b11a2",
      "metadata": {
        "id": "b73b11a2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# Charger les paramètres ou poids du modèle InceptionV3 pré-entraîné sur ImageNet\n",
        "modele_preentraine = InceptionV3(weights='imagenet',\n",
        "                                 input_shape = (HAUTEUR_IMAGE,LARGEUR_IMAGE,NOMBRE_CANAUX),\n",
        "                                 include_top = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e89445bf",
      "metadata": {
        "id": "e89445bf"
      },
      "outputs": [],
      "source": [
        "# modele_preentraine.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93555b7f",
      "metadata": {
        "id": "93555b7f"
      },
      "source": [
        "### Module d'extraction d'attributs visuels\n",
        "\n",
        "#### Détermination de la couche de sortie et de la couche où débute le peaufinage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3aab0bb",
      "metadata": {
        "id": "d3aab0bb"
      },
      "outputs": [],
      "source": [
        "modele_preentraine.trainable = True\n",
        "\n",
        "# Combien de couches se trouvent dans le modèle pré-entraîné\n",
        "nbr_couches_modele_preentraine = len(modele_preentraine.layers)\n",
        "print(\"Nombre de couches dans le modèle pré-entraîné d'origine: \", nbr_couches_modele_preentraine)\n",
        "\n",
        "# Comment obtenir l'index de la couche à partir de l'identifiant de la couche \n",
        "# https://www.thetopsites.net/article/50151157.shtml\n",
        "liste_noms_de_couche = [couche.name for couche in modele_preentraine.layers]\n",
        "nom_derniere_couche = liste_noms_de_couche[-1]\n",
        "print(\"Nom de la dernière couche du modèle pré-entraîné complet:\",nom_derniere_couche)\n",
        "index_derniere_couche = liste_noms_de_couche.index(nom_derniere_couche)\n",
        "print(\"Index de la dernière couche du modèle pré-entraîné complet:\",index_derniere_couche)\n",
        "\n",
        "# Choix d'une nouvelle couche de sortie par essai / erreur\n",
        "# cette étape assez laborieuse a été réalisée pour vous simplifier la tâche\n",
        "nom_derniere_couche = 'mixed5'\n",
        "print(\"Choix empirique de la dernière couche du modèle pré-entraîné:\",nom_derniere_couche)\n",
        "index_derniere_couche = liste_noms_de_couche.index(nom_derniere_couche)\n",
        "print(\"Index de la dernière couche du modèle pré-entraîné choisie empiriquement:\",index_derniere_couche)\n",
        "\n",
        "# Choix de la dernière couche non-entraînable ou dernière couche « gelée » du modèle pré-entraîné\n",
        "nom_derniere_couche_gelee = 'mixed3'\n",
        "print(\"Choix empirique de la dernière couche non-entraînable:\",nom_derniere_couche_gelee)\n",
        "index_derniere_couche_gelee = liste_noms_de_couche.index(nom_derniere_couche_gelee)\n",
        "print(\"Index de la dernière couche non-entraînable:\",index_derniere_couche_gelee)\n",
        "# Paufiner l'entraînement à partir de la dernière couche non-entraînable\n",
        "debut_paufinage = index_derniere_couche_gelee\n",
        "print(\"Nombre de couches gelées (non entraînables) dans le modèle préentraîné: \", debut_paufinage )\n",
        "# Geler tous les couches avant la couche `debut_paufinage`\n",
        "for couche in modele_preentraine.layers[:debut_paufinage]:\n",
        "    couche.trainable =  False\n",
        "\n",
        "print(\"Nombre de couches entraînables dans le modèle préentraîné: \", index_derniere_couche-debut_paufinage)\n",
        "derniere_couche = modele_preentraine.get_layer(nom_derniere_couche)\n",
        "print('Dimensions de la dernière couche: ', derniere_couche.output_shape)\n",
        "sortie_derniere_couche = derniere_couche.output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1d08a83",
      "metadata": {
        "id": "c1d08a83"
      },
      "source": [
        "### Ajout d'un module de classification\n",
        "#### Perceptron multicouche"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aedc48b5",
      "metadata": {
        "id": "aedc48b5"
      },
      "outputs": [],
      "source": [
        "# Ajouter une couche de régularisation par extinction de neurones (dropout)\n",
        "sorties = layers.Dropout(0.1)(sortie_derniere_couche)\n",
        "# Aplatir la couche de sortie en un vecteur (i.e. une dimension)\n",
        "sorties = layers.Flatten()(sorties)\n",
        "# Ajouter une couche entièrement connectée avec 1024 neurones cachés et l'activation ReLU\n",
        "sorties = layers.Dense(1024, activation='relu')(sorties)\n",
        "# Ajouter une couche de régularisation par extinction (dropout)\n",
        "sorties = layers.Dropout(0.2)(sorties)      \n",
        "# Ajouter une couche de sortie entièrement connectée de 20 neurones \n",
        "# avec l'activation softmax (exponentielle normalisée)\n",
        "# *** IMPORTANT *** 20 classes => 20 neurones de sortie\n",
        "sorties = layers.Dense(nombre_classes, activation='softmax')(sorties)           \n",
        "modele_de_transfert = Model(modele_preentraine.input, sorties) \n",
        "print(\"Nombre de couches dans le modèle d'apprentissage par transfert: \", len(modele_de_transfert.layers))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78916227",
      "metadata": {
        "id": "78916227"
      },
      "outputs": [],
      "source": [
        "# transfer_model.summary()\n",
        "print(\"Nombre de paramètres entraînables dans le modèle de transfert: \",len(modele_de_transfert.trainable_variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49680c6a",
      "metadata": {
        "id": "49680c6a"
      },
      "source": [
        "## Compilation du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a85e7c6a",
      "metadata": {
        "id": "a85e7c6a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# D'après la documentation, tf.keras.utils.image_dataset_from_directory, encodera par défaut \n",
        "# un ensemble d'étiquettes de classes sous forme d'entiers allant de 1 au nombre de classes.\n",
        "# Dans ce cas, le modèle doit être compilé avec une fonction d'erreur 'sparse_categorical_crossentropy'\n",
        "\n",
        "modele_de_transfert.compile(optimizer = RMSprop(learning_rate=0.0001),\n",
        "                            loss = 'sparse_categorical_crossentropy',\n",
        "                            metrics = ['accuracy'])\n",
        "\n",
        "print(\"Modèle compilé!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb1825da",
      "metadata": {
        "id": "fb1825da"
      },
      "source": [
        "## Entraînement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b177e7b",
      "metadata": {
        "id": "9b177e7b"
      },
      "outputs": [],
      "source": [
        "nombre_iterations_de_base = 10\n",
        "nombre_iterations_peaufinage = 30\n",
        "nombre_total_iterations =  nombre_iterations_de_base + nombre_iterations_peaufinage\n",
        "\n",
        "# Les fonctions de rappels sont passées au modèle via l'argument callbacks de .fit(...), \n",
        "# qui prend une liste de fonctions de rappels. \n",
        "liste_fonctions_rappels = [\n",
        "    # Mémoriser le meilleur modèle\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"IdEcorces_ResConv_TransAmpl.keras\",\n",
        "        save_best_only=True,\n",
        "        # Surveiller l'erreur de validation\n",
        "        monitor=\"val_loss\"),\n",
        "    # Arrêter l'entraînement quand son amélioration stagne\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        # Surveiller l'exactitude de validation\n",
        "        monitor='val_accuracy',\n",
        "        # Arrêter l'entraînement quand l'exactitude stagne\n",
        "        # depuis plus de 10 itérations (i.e., 11 itérations)\n",
        "        patience=10,\n",
        "        verbose=2),\n",
        "    # Ajuster le taux taux d'apprentissage ou gain du gradient\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        # Surveiller l'erreur de validation\n",
        "        monitor='val_loss',\n",
        "        # Diviser le taux d'apprentissage par 2 \n",
        "        # lorsque la fonction de rappel est déclenchée\n",
        "        factor=0.5,\n",
        "        # La fonction de rappel est déclenchée quand l'erreur de validation \n",
        "        # a cessé de s'améliorer depuis 3 itérations\n",
        "        patience=3,\n",
        "        verbose=2) \n",
        "]\n",
        "\n",
        "traces_entrainement = modele_de_transfert.fit(donnees_entrainement_normalisees,\n",
        "                                              validation_data = donnees_validation_normalisees,\n",
        "                                              epochs = nombre_total_iterations,\n",
        "                                              callbacks=liste_fonctions_rappels,\n",
        "                                              verbose = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa923e3d",
      "metadata": {
        "id": "fa923e3d"
      },
      "source": [
        "### Affichage des courbes d'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae216e3",
      "metadata": {
        "id": "fae216e3"
      },
      "outputs": [],
      "source": [
        "# Dernière couche 'mixed5', dernière couche gelée 'mixed3'\n",
        "# 40 itérations & amplification\n",
        "# xactitude sy=u les données de test: 80 à 85%\n",
        "import matplotlib.pyplot as plt\n",
        "exactitude_entrainement = traces_entrainement.history['accuracy']\n",
        "exactitude_validation = traces_entrainement.history['val_accuracy']\n",
        "erreur_entrainement = traces_entrainement.history['loss']\n",
        "erreur_validation = traces_entrainement.history['val_loss']\n",
        "nombre_iterations = range(len(exactitude_entrainement))\n",
        "nombre_dor = 1.618\n",
        "hauteur = 6\n",
        "longueur = int(nombre_dor * hauteur)\n",
        "plt.figure(figsize=(longueur,hauteur))\n",
        "plt.plot(nombre_iterations, exactitude_entrainement, 'r', label='Exactitude entraînement')\n",
        "plt.plot(nombre_iterations, exactitude_validation, 'b', label='Exactitude validation')\n",
        "plt.title(\"Exactitude en entraînement & validation - apprentissage par transfert & amplification des données\")\n",
        "plt.legend(loc=0)\n",
        "plt.xlabel(\"Nombre d'itérations\")\n",
        "plt.ylabel(\"Exactitude\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48bec051",
      "metadata": {
        "id": "48bec051"
      },
      "source": [
        "### Évaluation du modèle avec des données de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f73865d7",
      "metadata": {
        "id": "f73865d7"
      },
      "outputs": [],
      "source": [
        "erreur_test, exactitude_test = modele_de_transfert.evaluate(donnees_test_normalisees)\n",
        "print(\"Exactitude sur les données de test: %0.2f\" % exactitude_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a75e6dd",
      "metadata": {
        "id": "5a75e6dd"
      },
      "source": [
        "### Analyse des résultats avec les données de test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d0f83f9",
      "metadata": {
        "id": "6d0f83f9"
      },
      "source": [
        "#### Prédiction sur les données de test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ded27d",
      "metadata": {
        "id": "25ded27d"
      },
      "source": [
        "Extraction des étiquettes prédites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c25bca",
      "metadata": {
        "id": "97c25bca"
      },
      "outputs": [],
      "source": [
        "predictions = modele_de_transfert.predict(donnees_test_normalisees)\n",
        "index_predictions = np.argmax(predictions, axis=1)\n",
        "liste_etiquettes_predites = [(etiquette+1) for etiquette in index_predictions]\n",
        "print(liste_etiquettes_predites)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a1bb0a",
      "metadata": {
        "id": "91a1bb0a"
      },
      "source": [
        "Extraction des vraies étiquettes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50260255",
      "metadata": {
        "id": "50260255"
      },
      "outputs": [],
      "source": [
        "index_vraies_etiquettes = list(np.concatenate([etiquette for image, etiquette in donnees_test_normalisees], axis=0))\n",
        "liste_vraies_etiquettes = [(etiquette+1) for etiquette in index_vraies_etiquettes]\n",
        "print(liste_vraies_etiquettes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67e7151c",
      "metadata": {
        "id": "67e7151c"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "exactitude_test = metrics.accuracy_score(liste_vraies_etiquettes, liste_etiquettes_predites)\n",
        "print(\"Exactitude:   %0.2f\" % exactitude_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04158d6d",
      "metadata": {
        "id": "04158d6d"
      },
      "source": [
        "#### Affichage d'une matrice de confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "515b9d07",
      "metadata": {
        "id": "515b9d07"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/65618137/confusion-matrix-for-multiple-classes-in-python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "### Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# y_true=np.argmax(vraies_etiquettes_index, axis=-1)\n",
        "y_true= liste_vraies_etiquettes\n",
        "y_pred = liste_etiquettes_predites\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "## Get Class Labels\n",
        "class_names = liste_noms_classes\n",
        "\n",
        "# Plot confusion matrix in a beautiful manner\n",
        "fig = plt.figure(figsize=(16, 14))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Classes prédites', fontsize=20)\n",
        "ax.xaxis.set_label_position('bottom')\n",
        "plt.xticks(rotation=90)\n",
        "ax.xaxis.set_ticklabels(class_names, fontsize = 10)\n",
        "ax.xaxis.tick_bottom()\n",
        "\n",
        "ax.set_ylabel('Vraies classes', fontsize=20)\n",
        "ax.yaxis.set_ticklabels(class_names, fontsize = 10)\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "plt.title('Matrice de confusion', fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50522956",
      "metadata": {
        "id": "50522956"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Matrice de confusion',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    plt.figure(figsize=(14,12))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Matrice de confusion normalisée\")\n",
        "    else:\n",
        "        print('Matrice de confusion non normalisée')\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Vraies étiquettes')\n",
        "    plt.xlabel('Étiquettes prédites')\n",
        "\n",
        "cm = metrics.confusion_matrix(liste_vraies_etiquettes, liste_etiquettes_predites)\n",
        "plot_confusion_matrix(cm, classes=class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89de3ec0",
      "metadata": {
        "id": "89de3ec0"
      },
      "source": [
        "### Résultats\n",
        "\n",
        "Les résultats dans une fourchette de 82 à 85%. Avec 32 Go de données l'équipe de chercheurs de l'Université Laval à avait obtenu 93%. Mais avec 1.5 Go de données (21 fois moins de données), on ne peut pas faire de miracle... "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0209747c",
      "metadata": {
        "id": "0209747c"
      },
      "source": [
        "## Sauvegarde du modèle entraîné"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "489ce341",
      "metadata": {
        "id": "489ce341"
      },
      "outputs": [],
      "source": [
        "!mkdir \"modeles_sauvegardes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54433cfa",
      "metadata": {
        "id": "54433cfa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "chemin_sauvegarde = \"modeles_sauvegardes/\"\n",
        "\n",
        "# Ajouter une estampille temporelle (timestamp)\n",
        "estampille_temporelle = str(int(time.time()))\n",
        "chemin_modele_sauvegarde = os.path.join(chemin_sauvegarde, estampille_temporelle)\n",
        "tf.saved_model.save(modele_de_transfert, chemin_modele_sauvegarde)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40fb207d",
      "metadata": {
        "id": "40fb207d"
      },
      "source": [
        "## Télécharger le modèle entraîné\n",
        "\n",
        "Ici, vous pouvez télécharger le modèle entraîné sur votre poste de travail local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efb16181",
      "metadata": {
        "id": "efb16181"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"modele_sauvegarde\"+estampille_temporelle,'zip', chemin_modele_sauvegarde)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20fd891d",
      "metadata": {
        "id": "20fd891d"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"modele_sauvegarde_\"+estampille_temporelle+\".zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbaf2813",
      "metadata": {
        "id": "bbaf2813"
      },
      "outputs": [],
      "source": [
        "print(\"Fin de l'exécution du carnet IPython\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Id_Ecorces-Res_Conv-Transfert_Ampl-Colab.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}